{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad62500d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2451fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer, \n",
    "                          Seq2SeqTrainingArguments, Seq2SeqTrainer)\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bad2b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97087531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (C:/Users/ronna/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33318445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (C:/Users/ronna/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc11f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (C:/Users/ronna/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00655d5",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94961977",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a831eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7adc7",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc2ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 8192\n",
    "max_output_length = 512\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19855f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
    "    outputs = tokenizer(batch[\"abstract\"], padding=\"max_length\", truncation=True, max_length=max_output_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [[0 for _ in range(len(batch[\"input_ids\"][0]))]]\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955750d",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16238c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shards = 1000\n",
    "raw_sub_train_dataset = train_dataset.shard(num_shards=num_shards, index=random.randint(0, num_shards - 1))\n",
    "raw_sub_val_dataset = val_dataset.shard(num_shards=num_shards, index=random.randint(0, num_shards - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ba597",
   "metadata": {},
   "source": [
    "## Tokenize and Convert to Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e015e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "sub_train_dataset = raw_sub_train_dataset.map(process_data_to_model_inputs, batched=True, batch_size=batch_size, remove_columns=[\"article\", \"abstract\", \"section_names\"])\n",
    "sub_val_dataset = raw_sub_val_dataset.map(process_data_to_model_inputs, batched=True, batch_size=batch_size, remove_columns=[\"article\", \"abstract\", \"section_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024c0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"])\n",
    "sub_val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244753e1",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "e25a1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4a394",
   "metadata": {},
   "source": [
    "## Train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 33/180 1:13:53 < 5:50:23, 0.01 it/s, Epoch 0.53/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"allenai/led-base-16384_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"allenai/led-base-16384_logs\",\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=led,\n",
    "    args=training_args,\n",
    "    train_dataset=sub_train_dataset,\n",
    "    eval_dataset=sub_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50923dac",
   "metadata": {},
   "source": [
    "## Test Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dedef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "led.config.num_beams = 1\n",
    "led.config.max_length = 512\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(sub_val_dataset) - 1)\n",
    "sample = sub_val_dataset[random_index]\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "led.to(device)\n",
    "\n",
    "input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "global_attention_mask = sample[\"global_attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    summary_ids = led.generate(input_ids=input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "\n",
    "generated_summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "actual_summary = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "# Print and compare both summaries\n",
    "print(\"Generated Summary:\")\n",
    "print(generated_summary)\n",
    "print(\"\\nActual Summary:\")\n",
    "print(actual_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db17c1",
   "metadata": {},
   "source": [
    "# Train Variety of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"allenai/led-base-16384\"] # \"t5-small\", \"facebook/bart-small\", \"google/pegasus-small\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nTraining {model_name}\\n\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "    # Configure model settings\n",
    "    model.config.num_beams = 1\n",
    "    model.config.max_length = 512\n",
    "    model.config.min_length = 100\n",
    "    model.config.length_penalty = 2.0\n",
    "    model.config.early_stopping = True\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{model_name}_finetuned\",\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_dir=f\"{model_name}_logs\",\n",
    "        num_train_epochs=3,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=sub_train_dataset,\n",
    "        eval_dataset=sub_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f7b95",
   "metadata": {},
   "source": [
    "# LDA Exploration - Potential Baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265fc2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ronna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6169c53",
   "metadata": {},
   "source": [
    "## Individual Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7be7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = train_dataset[0][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812ce0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentences = sent_tokenize(article)\n",
    "single_word_tokenized_sentences = [word_tokenize(sent.lower()) for sent in single_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8042b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dictionary = Dictionary(single_word_tokenized_sentences)\n",
    "single_corpus = [single_dictionary.doc2bow(text) for text in single_word_tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cef7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_lda_model = LdaModel(single_corpus, num_topics=1, id2word=single_dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df836a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(single_sentences)\n",
    "single_lda_importances = single_lda_model.get_document_topics(single_corpus, minimum_probability=0)\n",
    "important_sentences = sorted(zip(range(len(single_sentences)), single_lda_importances), key=lambda x: -x[1][0][1])\n",
    "summary_length = 7\n",
    "summary_sentences = [single_sentences[i[0]].replace(\"\\n\", \" \") for i in important_sentences[:summary_length]]\n",
    "summary = ' '.join(summary_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4458e410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:\n",
      " background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran .   the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention . for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) .   however , there were no significant changes among boys or total population .   the mean of all anthropometric indices changed significantly after intervention both among girls and boys as well as in total population .   the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001).conclusion : this study demonstrates the potential success and scalability of school feeding programs in iran .   community nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children . \n",
      "\n",
      "Generated LDA Abstract:\n",
      "a recent systematic analysis showed that in 2011 , 314 ( 296 - 331 ) million children younger than 5 years were mildly , moderately or severely stunted and 258 ( 240 - 274 ) million were mildly , moderately or severely underweight in the developing countries . in iran a study among 752 high school girls in sistan and baluchestan showed prevalence of 16.2% , 8.6% and 1.5% , for underweight , overweight and obesity , respectively . the prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% . anthropometric study of elementary school students in shiraz revealed that 16% of them suffer from malnutrition and low body weight . snack should have 300 - 400 kcal energy and could provide 5 - 10 g of protein / day . nowadays , school nutrition programs are running as the national programs , world - wide . national school lunch program in the united states there are also some reports regarding school feeding programs in developing countries .\n"
     ]
    }
   ],
   "source": [
    "print(\"Abstract:\")\n",
    "print(train_dataset[0]['abstract'].replace(\"\\n\", \" \"))\n",
    "\n",
    "print(\"\\nGenerated LDA Abstract:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168fe3c",
   "metadata": {},
   "source": [
    "## Trained on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "066a5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = train_dataset[0][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a911d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(article):\n",
    "    sentences = sent_tokenize(article)\n",
    "    word_tokenized_sentences = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "    return word_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a1093db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train_dataset = train_dataset.shard(num_shards=10, index=random.randint(0, 10 - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d8ff08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = []\n",
    "for data in lda_train_dataset:\n",
    "    article_sentences = preprocess_data(data[\"article\"])\n",
    "    train_corpus.extend(article_sentences)\n",
    "\n",
    "dictionary = Dictionary(train_corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in train_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d52e59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 1\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74a0967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_get_top_n_sentences(article, lda_model, dictionary, n=10):\n",
    "    sentences = sent_tokenize(article)\n",
    "    word_tokenized_sentences = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "\n",
    "    sentence_topic_distributions = [lda_model.get_document_topics(dictionary.doc2bow(sentence)) for sentence in word_tokenized_sentences]\n",
    "    sorted_sentences = sorted(enumerate(sentence_topic_distributions), key=lambda x: x[1][0][1], reverse=True)\n",
    "    top_n_sentences_indices = [sentence_info[0] for sentence_info in sorted_sentences[:n]]\n",
    "    top_n_sentences = [sentences[idx] for idx in top_n_sentences_indices]\n",
    "\n",
    "    return top_n_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1607f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_sentences = lda_get_top_n_sentences(test_article, lda_model, dictionary)\n",
    "summary = \" \".join(important_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7021074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:\n",
      " background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran .   the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention . for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) .   however , there were no significant changes among boys or total population .   the mean of all anthropometric indices changed significantly after intervention both among girls and boys as well as in total population .   the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001).conclusion : this study demonstrates the potential success and scalability of school feeding programs in iran .   community nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children . \n",
      "\n",
      "Generated LDA Abstract:\n",
      "a recent systematic analysis showed that in 2011 , 314 ( 296 - 331 ) million children younger than 5 years were mildly , moderately or severely stunted and 258 ( 240 - 274 ) million were mildly , moderately or severely underweight in the developing countries . in iran a study among 752 high school girls in sistan and baluchestan showed prevalence of 16.2% , 8.6% and 1.5% , for underweight , overweight and obesity , respectively . the prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% . anthropometric study of elementary school students in shiraz revealed that 16% of them suffer from malnutrition and low body weight . snack should have 300 - 400 kcal energy and could provide 5 - 10 g of protein / day . nowadays , school nutrition programs are running as the national programs , world - wide . national school lunch program in the united states\n",
      "there are also some reports regarding school feeding programs in developing countries . in vietnam ,\n",
      "school base program showed an improvement in nutrient intakes . in iran a national free food program ( nffp )\n",
      "is implemented in elementary schools of deprived areas to cover all poor students . however , this program is not conducted in slums and poor areas of the big cities so many malnourished children with low socio - economic situation are not covered by nffp .\n"
     ]
    }
   ],
   "source": [
    "print(\"Abstract:\")\n",
    "print(train_dataset[0]['abstract'].replace(\"\\n\", \" \"))\n",
    "\n",
    "print(\"\\nGenerated LDA Abstract:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf92f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa261679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_sentences(article, topic_distributions, n=10):\n",
    "    sentences = sent_tokenize(article)\n",
    "    sorted_sentences = sorted(enumerate(topic_distributions), key=lambda x: x[1][0], reverse=True)\n",
    "    top_n_sentences_indices = [sentence_info[0] for sentence_info in sorted_sentences[:n]]\n",
    "    top_n_sentences = [sentences[idx] for idx in top_n_sentences_indices]\n",
    "    return top_n_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6231c5",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization (NMF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1bfc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "173c24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nmf_model(train_corpus, num_topics=1):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform([' '.join(text) for text in train_corpus])\n",
    "    \n",
    "    nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "    nmf_model.fit(X)\n",
    "    \n",
    "    return nmf_model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "459f14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model, vectorizer = create_nmf_model(train_corpus, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd3f01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topic_distribution(article_sentences, nmf_model, vectorizer):\n",
    "    X = vectorizer.transform([' '.join(text) for text in article_sentences])\n",
    "    topic_distribution = nmf_model.transform(X)\n",
    "    return topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e169c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated NMF Abstract:\n",
      "there was also a significant increase in the proportion of children with bmi that was normal for age ( 2 to + 1 sd ) most of the published community interventions showed better results among females compared with males . the mean of age in welfare group was 10.0  2.3 and 10.5  2.5 in non - welfare group . the results of the mentioned study showed an improvement in the weight of children , psychological test 's scores and the grade - point average following this school feeding program . the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001 ) . bmi for age for iranian students aged 7 - 14 years based on gender according to who growth standards 2007 bmi for age for iranian students aged 7 - 14 years according to who growth standards 2007 in non - welfare and welfare groups of total population table 4 has shown the prevalence of normal bmi , mild , moderate and severe malnutrition in non - welfare and welfare groups of school aged children separately among boys and girls before and after a nutrition intervention based on advocacy process model . when we assess the effect of intervention in total population without separating by sex groups , we found no significant change in this population [ table 3 ] . in order to determine the effective variables on the malnutrition status paired t test was used to compare the end values with baseline ones in each group . in order to determine the effective variables on the malnutrition status paired t test was used to compare the end values with baseline ones in each group . 19.5% of subjects were in case group ( n = 561 ) and 80.5% were in the control group ( n = 2336 ) . this study shows determinant factor of nutritional status of school age children was their socio - economic level .\n"
     ]
    }
   ],
   "source": [
    "nmf_topic_distribution = get_nmf_topic_distribution(preprocess_data(test_article), nmf_model, vectorizer)\n",
    "important_sentences = get_top_n_sentences(test_article, nmf_topic_distribution, n=10)\n",
    "nmf_summary = \" \".join(important_sentences)\n",
    "print(\"\\nGenerated NMF Abstract:\")\n",
    "print(nmf_summary.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31b103",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA) or Latent Semantic Indexing (LSI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b13ec33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "829cb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lsa_model(train_corpus, num_topics=1):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform([' '.join(text) for text in train_corpus])\n",
    "\n",
    "    lsa_model = TruncatedSVD(n_components=num_topics, random_state=42)\n",
    "    lsa_model.fit(X)\n",
    "\n",
    "    return lsa_model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b829b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model, vectorizer = create_lsa_model(train_corpus, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa9c4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsa_topic_distribution(article_sentences, lsa_model, vectorizer):\n",
    "    X = vectorizer.transform([' '.join(text) for text in article_sentences])\n",
    "    topic_distribution = lsa_model.transform(X)\n",
    "    return topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c085a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated LSA Abstract:\n",
      "there was also a significant increase in the proportion of children with bmi that was normal for age ( 2 to + 1 sd ) most of the published community interventions showed better results among females compared with males . the mean of age in welfare group was 10.0  2.3 and 10.5  2.5 in non - welfare group . the results of the mentioned study showed an improvement in the weight of children , psychological test 's scores and the grade - point average following this school feeding program . the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001 ) . bmi for age for iranian students aged 7 - 14 years based on gender according to who growth standards 2007 bmi for age for iranian students aged 7 - 14 years according to who growth standards 2007 in non - welfare and welfare groups of total population table 4 has shown the prevalence of normal bmi , mild , moderate and severe malnutrition in non - welfare and welfare groups of school aged children separately among boys and girls before and after a nutrition intervention based on advocacy process model . when we assess the effect of intervention in total population without separating by sex groups , we found no significant change in this population [ table 3 ] . in order to determine the effective variables on the malnutrition status paired t test was used to compare the end values with baseline ones in each group . in order to determine the effective variables on the malnutrition status paired t test was used to compare the end values with baseline ones in each group . 19.5% of subjects were in case group ( n = 561 ) and 80.5% were in the control group ( n = 2336 ) . this study shows determinant factor of nutritional status of school age children was their socio - economic level .\n"
     ]
    }
   ],
   "source": [
    "lsa_topic_distribution = get_lsa_topic_distribution(preprocess_data(test_article), lsa_model, vectorizer)\n",
    "important_sentences = get_top_n_sentences(test_article, lsa_topic_distribution, n=10)\n",
    "lsa_summary = \" \".join(important_sentences)\n",
    "print(\"\\nGenerated LSA Abstract:\")\n",
    "print(lsa_summary.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49c5a6",
   "metadata": {},
   "source": [
    "# Hierarchical Dirichlet Process (HDP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58ebcbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74c6cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_model = hdp_model = HdpModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c871719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdp_topic_distribution(article_sentences, hdp_model, dictionary):\n",
    "    bow = [dictionary.doc2bow(sent) for sent in article_sentences]\n",
    "    topic_distribution = [hdp_model[c] for c in bow]\n",
    "    return topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "772eeadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated HDP Abstract:\n",
      "in general , the new snack package in average has provided 380 kcal energy , 15 g protein along with sufficient calcium and iron . in general , the new snack package in average has provided 380 kcal energy , 15 g protein along with sufficient calcium and iron . in general , the new snack package in average has provided 380 kcal energy , 15 g protein along with sufficient calcium and iron . statistical analyses were performed using the statistical package for the social sciences ( spss ) software , version 17.0 ( spss inc . for implementing the interventions , weight was to the nearest 0.1 kg on a balance scale ( model # seca scale ) . statistical analyses were performed using the statistical package for the social sciences ( spss ) software , version 17.0 ( spss inc . snack should have 300 - 400 kcal energy and could provide 5 - 10 g of protein / day . standing height was measured to the nearest 0.1 cm with a wall - mounted stadiometer . standing height was measured to the nearest 0.1 cm with a wall - mounted stadiometer . , chicago , il , usa ) .\n"
     ]
    }
   ],
   "source": [
    "hdp_topic_distribution = get_hdp_topic_distribution(preprocess_data(test_article), hdp_model, dictionary)\n",
    "important_sentences = get_top_n_sentences(test_article, hdp_topic_distribution, n=10)\n",
    "hdp_summary = \" \".join(important_sentences)\n",
    "print(\"\\nGenerated HDP Abstract:\")\n",
    "print(hdp_summary.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da16c55",
   "metadata": {},
   "source": [
    "# Correlation Explanation (CorEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62256cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corextopic import corextopic as ct\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d186afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corex_model(train_corpus, num_topics=1):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform([' '.join(text) for text in train_corpus])\n",
    "    \n",
    "    corex_model = ct.Corex(n_hidden=num_topics, words=vectorizer.get_feature_names_out(), seed=42)\n",
    "    corex_model.fit(X, words=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return corex_model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a1a8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "corex_model, vectorizer = create_corex_model(train_corpus, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b90e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corex_topic_distribution(article_sentences, corex_model, vectorizer):\n",
    "    X = vectorizer.transform([' '.join(text) for text in article_sentences])\n",
    "    topic_distribution = corex_model.transform(X)\n",
    "    return topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06b87024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated CorEx Abstract:\n",
      ", chicago , il , usa ) . a recent systematic analysis showed that in 2011 , 314 ( 296 - 331 ) million children younger than 5 years were mildly , moderately or severely stunted and 258 ( 240 - 274 ) million were mildly , moderately or severely underweight in the developing countries . in iran a study among 752 high school girls in sistan and baluchestan showed prevalence of 16.2% , 8.6% and 1.5% , for underweight , overweight and obesity , respectively . the prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% . anthropometric study of elementary school students in shiraz revealed that 16% of them suffer from malnutrition and low body weight . snack should have 300 - 400 kcal energy and could provide 5 - 10 g of protein / day . nowadays , school nutrition programs are running as the national programs , world - wide . national school lunch program in the united states there are also some reports regarding school feeding programs in developing countries . in vietnam , school base program showed an improvement in nutrient intakes . in iran a national free food program ( nffp ) is implemented in elementary schools of deprived areas to cover all poor students .\n"
     ]
    }
   ],
   "source": [
    "corex_topic_distribution = get_corex_topic_distribution(preprocess_data(test_article), corex_model, vectorizer)\n",
    "important_sentences = get_top_n_sentences(test_article, corex_topic_distribution, n=10)\n",
    "corex_summary = \" \".join(important_sentences)\n",
    "print(\"\\nGenerated CorEx Abstract:\")\n",
    "print(corex_summary.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e30608",
   "metadata": {},
   "source": [
    "## Accuracy Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81169d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "658c6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "def evaluate_summary(generated_summary, actual_summary):\n",
    "    rouge_scores = rouge.get_scores(generated_summary, actual_summary, avg=True)\n",
    "    \n",
    "    bleu_score = sentence_bleu([actual_summary.split()], generated_summary.split())    \n",
    "    return rouge_scores, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd017b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_scores(rouge_scores_list, bleu_scores_list):\n",
    "    rouge_avg = {'rouge-1': {'f': 0, 'p': 0, 'r': 0},\n",
    "                 'rouge-2': {'f': 0, 'p': 0, 'r': 0},\n",
    "                 'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
    "    bleu_avg = 0\n",
    "    \n",
    "    n = len(rouge_scores_list)\n",
    "    \n",
    "    for rouge_scores, bleu_score in zip(rouge_scores_list, bleu_scores_list):\n",
    "        for key in rouge_avg.keys():\n",
    "            rouge_avg[key]['f'] += rouge_scores[key]['f']\n",
    "            rouge_avg[key]['p'] += rouge_scores[key]['p']\n",
    "            rouge_avg[key]['r'] += rouge_scores[key]['r']\n",
    "        bleu_avg += bleu_score\n",
    "        \n",
    "    for key in rouge_avg.keys():\n",
    "        rouge_avg[key]['f'] /= n\n",
    "        rouge_avg[key]['p'] /= n\n",
    "        rouge_avg[key]['r'] /= n\n",
    "    \n",
    "    bleu_avg /= n\n",
    "    \n",
    "    return rouge_avg, bleu_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "973329c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge-1': {'r': 0.3026315789473684, 'p': 0.33093525179856115, 'f': 0.3161511977591196}, 'rouge-2': {'r': 0.05752212389380531, 'p': 0.058823529411764705, 'f': 0.05816554309906004}, 'rouge-l': {'r': 0.2894736842105263, 'p': 0.31654676258992803, 'f': 0.3024054932917656}}\n",
      "BLEU Score: 0.03837598605141113\n"
     ]
    }
   ],
   "source": [
    "rouge_scores, bleu_score = evaluate_summary(summary, train_dataset[0]['abstract'].replace(\"\\n\", \" \"))\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94731863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_lda_scores(generate_summary_func, model, vectorizer=None, dictionary=None, n=10):\n",
    "    rouge_scores_list = []\n",
    "    bleu_scores_list = []\n",
    "\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "        article = test_dataset[i]['article']\n",
    "        actual_summary = test_dataset[i]['abstract'].replace(\"\\n\", \" \")\n",
    "        \n",
    "        generated_summary = \" \".join(generate_summary_func(article, model, dictionary)).replace('\\n', ' ')\n",
    "\n",
    "        rouge_scores, bleu_score = evaluate_summary(generated_summary, actual_summary)\n",
    "\n",
    "        rouge_scores_list.append(rouge_scores)\n",
    "        bleu_scores_list.append(bleu_score)\n",
    "\n",
    "    return rouge_scores_list, bleu_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef737108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_scores(rouge_scores_list, bleu_scores_list):\n",
    "    rouge_avg = {'rouge-1': {'f': 0, 'p': 0, 'r': 0},\n",
    "                 'rouge-2': {'f': 0, 'p': 0, 'r': 0},\n",
    "                 'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
    "    bleu_avg = 0\n",
    "    \n",
    "    n = len(rouge_scores_list)\n",
    "    \n",
    "    for rouge_scores, bleu_score in zip(rouge_scores_list, bleu_scores_list):\n",
    "        for key in rouge_avg.keys():\n",
    "            rouge_avg[key]['f'] += rouge_scores[key]['f']\n",
    "            rouge_avg[key]['p'] += rouge_scores[key]['p']\n",
    "            rouge_avg[key]['r'] += rouge_scores[key]['r']\n",
    "        bleu_avg += bleu_score\n",
    "        \n",
    "    for key in rouge_avg.keys():\n",
    "        rouge_avg[key]['f'] /= n\n",
    "        rouge_avg[key]['p'] /= n\n",
    "        rouge_avg[key]['r'] /= n\n",
    "    \n",
    "    bleu_avg /= n\n",
    "    \n",
    "    return rouge_avg, bleu_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3f3abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6658 [00:00<?, ?it/s]C:\\Users\\ronna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\ronna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  0%|                                                                                 | 6/6658 [00:00<04:30, 24.62it/s]C:\\Users\\ronna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6658/6658 [05:08<00:00, 21.59it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_rouge_scores, lda_bleu_scores = compute_lda_scores(lda_get_top_n_sentences, lda_model, dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c961ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_avg_rouge, lda_avg_bleu = average_scores(lda_rouge_scores, lda_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee6d6b41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA - Average ROUGE and BLEU scores:\n",
      "ROUGE: {'rouge-1': {'f': 0.3347642736560891, 'p': 0.29099570991104895, 'r': 0.4259447185029159}, 'rouge-2': {'f': 0.13646877750930897, 'p': 0.11753033859174543, 'r': 0.1822507674043246}, 'rouge-l': {'f': 0.30466327865932413, 'p': 0.2650233061628383, 'r': 0.38745501151441414}}\n",
      "BLEU: 0.09292752245553795\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA - Average ROUGE and BLEU scores:\")\n",
    "print(\"ROUGE:\", lda_avg_rouge)\n",
    "print(\"BLEU:\", lda_avg_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7a26e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(generate_summary_func, model, vectorizer=None, dictionary=None, n=10):\n",
    "    rouge_scores_list = []\n",
    "    bleu_scores_list = []\n",
    "\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "        article = test_dataset[i]['article']\n",
    "        actual_summary = test_dataset[i]['abstract'].replace(\"\\n\", \" \")\n",
    "        \n",
    "        generated_summary = \" \".join(generate_summary_func(article, model, vectorizer, dictionary, n)).replace('\\n', ' ')\n",
    "\n",
    "        rouge_scores, bleu_score = evaluate_summary(generated_summary, actual_summary)\n",
    "\n",
    "        rouge_scores_list.append(rouge_scores)\n",
    "        bleu_scores_list.append(bleu_score)\n",
    "\n",
    "    return rouge_scores_list, bleu_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30fa26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6658 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_top_n_sentences() takes from 2 to 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nmf_rouge_scores, nmf_bleu_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_top_n_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m lsa_rouge_scores, lsa_bleu_scores \u001b[38;5;241m=\u001b[39m compute_scores(get_top_n_sentences, lsa_model, vectorizer\u001b[38;5;241m=\u001b[39mvectorizer)\n\u001b[0;32m      3\u001b[0m hdp_rouge_scores, hdp_bleu_scores \u001b[38;5;241m=\u001b[39m compute_scores(get_top_n_sentences, hdp_model, dictionary\u001b[38;5;241m=\u001b[39mdictionary)\n",
      "Cell \u001b[1;32mIn[86], line 9\u001b[0m, in \u001b[0;36mcompute_scores\u001b[1;34m(generate_summary_func, model, vectorizer, dictionary, n)\u001b[0m\n\u001b[0;32m      6\u001b[0m article \u001b[38;5;241m=\u001b[39m test_dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m actual_summary \u001b[38;5;241m=\u001b[39m test_dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m generated_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mgenerate_summary_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m rouge_scores, bleu_score \u001b[38;5;241m=\u001b[39m evaluate_summary(generated_summary, actual_summary)\n\u001b[0;32m     13\u001b[0m rouge_scores_list\u001b[38;5;241m.\u001b[39mappend(rouge_scores)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_top_n_sentences() takes from 2 to 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "nmf_rouge_scores, nmf_bleu_scores = compute_scores(get_top_n_sentences, nmf_model, vectorizer=vectorizer)\n",
    "lsa_rouge_scores, lsa_bleu_scores = compute_scores(get_top_n_sentences, lsa_model, vectorizer=vectorizer)\n",
    "hdp_rouge_scores, hdp_bleu_scores = compute_scores(get_top_n_sentences, hdp_model, dictionary=dictionary)\n",
    "corex_rouge_scores, corex_bleu_scores = compute_scores(get_top_n_sentences, corex_model, vectorizer=vectorizer)\n",
    "\n",
    "nmf_avg_rouge, nmf_avg_bleu = average_scores(nmf_rouge_scores, nmf_bleu_scores)\n",
    "lsa_avg_rouge, lsa_avg_bleu = average_scores(lsa_rouge_scores, lsa_bleu_scores)\n",
    "hdp_avg_rouge, hdp_avg_bleu = averagee_scores(hdp_rouge_scores, hdp_bleu_scores)\n",
    "corex_avg_rouge, corex_avg_bleu = average_scores(corex_rouge_scores, corex_bleu_scores)\n",
    "\n",
    "print(\"\\nNMF - Average ROUGE and BLEU scores:\")\n",
    "print(\"ROUGE:\", nmf_avg_rouge)\n",
    "print(\"BLEU:\", nmf_avg_bleu)\n",
    "\n",
    "print(\"\\nLSA - Average ROUGE and BLEU scores:\")\n",
    "print(\"ROUGE:\", lsa_avg_rouge)\n",
    "print(\"BLEU:\", lsa_avg_bleu)\n",
    "\n",
    "print(\"\\nHDP - Average ROUGE and BLEU scores:\")\n",
    "print(\"ROUGE:\", hdp_avg_rouge)\n",
    "print(\"BLEU:\", hdp_avg_bleu)\n",
    "\n",
    "print(\"\\nCorEx - Average ROUGE and BLEU scores:\")\n",
    "print(\"ROUGE:\", corex_avg_rouge)\n",
    "print(\"BLEU:\", corex_avg_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d62a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
