{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9eb30c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c490daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer, \n",
    "                          Seq2SeqTrainingArguments, Seq2SeqTrainer)\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f837c4f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97087531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (/home/u_51520750/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33318445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (/home/u_51520750/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b10845ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset scientific_papers (/home/u_51520750/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e68beb",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebf4b1",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a831eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "705e7d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fe8b3",
   "metadata": {},
   "source": [
    "## Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fc2ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 8192\n",
    "max_output_length = 512\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963db71e",
   "metadata": {},
   "source": [
    "# Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a19855f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
    "    outputs = tokenizer(batch[\"abstract\"], padding=\"max_length\", truncation=True, max_length=max_output_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [[0 for _ in range(len(batch[\"input_ids\"][0]))]]\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e3bd3",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbf10a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shards = 1000\n",
    "raw_sub_train_dataset = train_dataset.shard(num_shards=num_shards, index=random.randint(0, num_shards - 1))\n",
    "raw_sub_val_dataset = val_dataset.shard(num_shards=num_shards, index=random.randint(0, num_shards - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8b01d",
   "metadata": {},
   "source": [
    "## Tokenize and Convert to Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e015e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "sub_train_dataset = raw_sub_train_dataset.map(process_data_to_model_inputs, batched=True, batch_size=batch_size, remove_columns=[\"article\", \"abstract\", \"section_names\"])\n",
    "sub_val_dataset = raw_sub_val_dataset.map(process_data_to_model_inputs, batched=True, batch_size=batch_size, remove_columns=[\"article\", \"abstract\", \"section_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8755bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"])\n",
    "sub_val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808341a0",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "694a612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4569af7",
   "metadata": {},
   "source": [
    "## Train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78aae6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 11:22:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.870500</td>\n",
       "      <td>5.495304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.777000</td>\n",
       "      <td>5.290465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.363900</td>\n",
       "      <td>5.199156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=5.003811815049914, metrics={'train_runtime': 41165.0255, 'train_samples_per_second': 0.009, 'train_steps_per_second': 0.004, 'total_flos': 1944147481067520.0, 'train_loss': 5.003811815049914, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"allenai/led-base-16384_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"allenai/led-base-16384_logs\",\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=led,\n",
    "    args=training_args,\n",
    "    train_dataset=sub_train_dataset,\n",
    "    eval_dataset=sub_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3714ea",
   "metadata": {},
   "source": [
    "# Save Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40aad4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "led.save_pretrained('model1_gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574c258",
   "metadata": {},
   "source": [
    "## Test Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4dedef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "led.config.num_beams = 1\n",
    "led.config.max_length = 512\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4264f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(sub_val_dataset) - 1)\n",
    "sample = sub_val_dataset[random_index]\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "led.to(device)\n",
    "\n",
    "input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "global_attention_mask = sample[\"global_attention_mask\"].unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "039a04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary\n",
    "with torch.no_grad():\n",
    "    summary_ids = led.generate(input_ids=input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a16f37c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "# background : a ( 4.8% versus 2.8i, 2. ne%, canada. \n",
      " the a lumbar spine surgery at increased risk of postoperative complications, as defined by reoperation, within 3 months of the index procedure.  \n",
      " a ( a ( ) ) 35 undergoing elective lumb them spine surgery, cans.  Arsenal a ( the a (mi ) 35onal elective and unanticipated reoperation ( the rate of reoperation and non - obese subjects.  per the a bmi 35, corresponding to class ii and iii obesity as per the world health organization classification system.  det a ( 3% ), and type of procedure performed ( decompression, decompression with instrumented fusion, deformity correction, or arthroplasty ) other confounding factors, such as medical comorbidities and smoking, were not available through this database.  the a all patients would be in this dataset, unless they traveled out of the province for care.  a  captive patient population allows for powerful large group analyses. in the a was no ability from the billing records to control for other comor overwhelmingities such as smoking, diabetes, chronic obstructive pulmonary disease, drug abuse, or osteoporosis?\n",
      ". if multiple risk factors are available in a dataset, a multivariate analysis can be helpful, included gender, age, location of surgery ( urban versus rural setting ), the a on the a n provides 35, includedlicted, age.  ( x1.96/n ).  was determined ( rr  =  risk of reasty, for obese subjects and the 95% confidence interval ( ci ) was determined with the canadian national average ( 7.8%).11\n",
      "five percent of obese subjects required early reoperation after elective was noar spine outstanding. compared with non - - obese half, this corresponds to a point rr of 1.73. compared, deform standards correction is associated with the largest risk for early re CASE, and non and to a ( provides ) 35 Fant elective ( c% ) wasbas ( raut  = ( risk of thoperation, for both obese and non-------- obese subjects, thisung to aive l01ar spine ). compared. m not have the benefit of longer - term follow - up.. a ( 1. was the a higher risk of these studies came close to the presented study population, the message is the same : expect significantly higher complication rates\n"
     ]
    }
   ],
   "source": [
    "generated_summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated Summary:\")\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "026af9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Summary:\n",
      " study design :  population - based retrospective cohort study.clinical question :  are patients with a body mass index ( bmi ) of 35 or more who undergo elective lumbar spine surgery at increased risk of post - surgical complications , as evidenced by reoperation within a 3-month period?methods :  the alberta health and wellness administrative database was queried to identify patients who underwent elective lumbar spine surgery over a 24-month period . \n",
      " this same database was used to classify subjects as obese ( bmi 35 ) and non - obese ( bmi < 35 ) and to determine who underwent repeated surgical intervention . \n",
      " the rate of reoperation was determined for both the obese and non - obese groups ; further analyses were performed to determine whether certain subjects were at increased risk of reoperation.results :  the point estimate for relative risk for requiring reoperation was 1.73 ( 95% confidence interval , 1.032.90 ) for obese subjects compared with non - obese subjects . \n",
      " the adjusted point estimate shows that deformity correction surgery is predictive for early reoperation while obesity is not.conclusions :  in obese subjects we observed an increased complication rate after elective lumbar spine surgery , as evidenced by reoperation rates within 3 months . \n",
      " when we considered other possible associations with reoperation , in adjusted analysis , deformity surgery was found to be predictive of early reoperation.final class of evidence - prognosisstudy designprospective cohortretrospective cohortcase controlcase seriesmethodspatients at similar point in course of treatmentf / u \n",
      "  85%similarity of treatment protocols for patient groupspatients followed up long enough for outcomes to occurcontrol for extraneous risk factorsoverall class of evidenceiiithe definiton of the different classes of evidence is available on page 55 . \n"
     ]
    }
   ],
   "source": [
    "actual_summary = raw_sub_val_dataset[random_index][\"abstract\"]\n",
    "print(\"Actual Summary:\")\n",
    "print(actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2ca0562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2050,  1486,  1058,   220,  3265,   532,  1912, 41432, 20812,  2050,\n",
       "           13, 47367,  1808,  1058,   220,   389,  3871,   351,   257,  1767,\n",
       "         2347,  6376,   357,   275, 11632,  1267,   286,  3439,   393,   517,\n",
       "          508, 17777,  1742,   425,   300,  2178,   283, 19656,  8185,   379,\n",
       "         3220,  2526,   286,  1281,   532, 21998, 19481,   837,   355, 30204,\n",
       "          416,   302, 27184,  1626,   257,   513,    12,  8424,  2278,    30,\n",
       "        24396,    82,  1058,   220,   262,   435,  4835,    64,  1535,   290,\n",
       "        42506, 11553,  6831,   373, 42517,   798,   284,  5911,  3871,   508,\n",
       "        25289,  1742,   425,   300,  2178,   283, 19656,  8185,   625,   257,\n",
       "         1987,    12,  8424,  2278,   764,   220,   198,   428,   976,  6831,\n",
       "          373,   973,   284, 36509,  7481,   355, 20779,   357,   275, 11632,\n",
       "         3439,  1267,   290,  1729,   532, 20779,   357,   275, 11632,  1279,\n",
       "         3439,  1267,   290,   284,  5004,   508, 25289,  5100, 21998,  9572,\n",
       "          764,   220,   198,   262,  2494,   286,   302, 27184,   373,  5295,\n",
       "          329,  1111,   262, 20779,   290,  1729,   532, 20779,  2628,  2162,\n",
       "         2252, 13523,   547,  6157,   284,  5004,  1771,  1728,  7481,   547,\n",
       "          379,  3220,  2526,   286,   302, 27184,    13, 43420,  1058,   220,\n",
       "          262,   966,  8636,   329,  3585,  2526,   329, 10616,   302, 27184,\n",
       "          373,   352,    13,  4790,   357,  6957,     4,  6628, 16654,   837,\n",
       "          352,    13, 49959,    13,  3829,  1267,   329, 20779,  7481,  3688,\n",
       "          351,  1729,   532, 20779,  7481,   764,   220,   198,   262, 12328,\n",
       "          966,  8636,  2523,   326, 47577,   414, 17137,  8185,   318, 33344,\n",
       "          329,  1903,   302, 27184,   981, 13825,   318,   407,    13,  1102,\n",
       "        11539,  1058,   220,   287, 20779,  7481,   356,  6515,   281,  3220,\n",
       "        45185,  2494,   706,  1742,   425,   300,  2178,   283, 19656,  8185,\n",
       "          837,   355, 30204,   416,   302, 27184,  3965,  1626,   513,  1933,\n",
       "          764,   220,   198,   618,   356,  3177,   584,  1744, 15814,   351,\n",
       "          302, 27184,   837,   287, 12328,  3781,   837, 47577,   414,  8185,\n",
       "          373,  1043,   284,   307, 33344,   286,  1903,   302, 27184,    13,\n",
       "        20311,  1398,   286,  2370,   532,  1172, 31707, 44517,  1486,  1676,\n",
       "        49540, 20812,  1186,   305, 49540, 20812,  7442,  1630,  7442,  2168,\n",
       "        24396,  2777,   265,  2334,   379,  2092,   966,   287,  1781,   286,\n",
       "         3513,    69,  1220,   334,   220,   198,   220,  7600,     4, 38610,\n",
       "          414,   286,  3513, 19565,   329,  5827,  2628,  8071,  2334,  3940,\n",
       "          510,   890,  1576,   329, 10906,   284,  3051, 13716,   329, 22820,\n",
       "        11655,  2526,  5766,   568,   332,   439,  1398,   286,  2370,  4178,\n",
       "        31470,  2730, 37752,   286,   262,  1180,  6097,   286,  2370,   318,\n",
       "         1695,   319,  2443,  5996,   764,   220,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "867d6fff",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m actual_summary \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Print and compare both summaries\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mActual Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/cs109b/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3486\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3483\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3484\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/cs109b/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:549\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    548\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 549\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    552\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "actual_summary = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "# Print and compare both summaries\n",
    "print(\"\\nActual Summary:\")\n",
    "print(actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "actual_summary = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "# Print and compare both summaries\n",
    "print(\"Generated Summary:\")\n",
    "print(generated_summary)\n",
    "print(\"\\nActual Summary:\")\n",
    "print(actual_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
