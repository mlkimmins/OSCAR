{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### optimized for python 3.8 - Pytorch and Tensorflow kernel on Azure on mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04 base docker image. Ensure right kernel"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e87c8f4a-6ec8-4e15-a6ba-229bede853c2"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-05-08 22:13:35.342503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-08 22:13:41.103911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584027667
        }
      },
      "id": "2d554205-b48e-4224-8ee7-09baa6c11e38"
    },
    {
      "cell_type": "code",
      "source": [
        "%conda install -c conda-forge -y ipywidgets"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Retrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\nSolving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n\n## Package Plan ##\n\n  environment location: /anaconda/envs/azureml_py38_PT_TF\n\n  added / updated specs:\n    - ipywidgets\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2023.5.7   |       hbcca054_0         145 KB  conda-forge\n    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         294 KB\n\nThe following packages will be UPDATED:\n\n  ca-certificates                      2022.12.7-ha878542_0 --> 2023.5.7-hbcca054_0 \n  certifi                            2022.12.7-pyhd8ed1ab_0 --> 2023.5.7-pyhd8ed1ab_0 \n\n\n\nDownloading and Extracting Packages\nca-certificates-2023 | 145 KB    |                                       |   0% \nca-certificates-2023 | 145 KB    | ##################################### | 100% \u001b[A\ncertifi-2023.5.7     | 149 KB    | ###9                                  |  11% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\nPreparing transaction: / \b\bdone\nVerifying transaction: \\ \b\bdone\nExecuting transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1682373564392
        }
      },
      "id": "a5154706-e6ba-45fa-be53-ace0e88f08c8"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install rouge-score"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: rouge-score in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (0.1.2)\nRequirement already satisfied: nltk in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from rouge-score) (3.8.1)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: absl-py in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from nltk->rouge-score) (1.2.0)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from nltk->rouge-score) (2023.5.5)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from nltk->rouge-score) (4.65.0)\nRequirement already satisfied: click in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from nltk->rouge-score) (8.1.3)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "c78e924f-50ad-443a-afcb-fe3ca7483b1b"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pip install transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pip in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (22.3.1)\nRequirement already satisfied: install in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (1.3.5)\nRequirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (4.28.1)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (22.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "000b5053-ebdd-4c3b-9a0d-d21c920aa891"
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install tensorflow_addons"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584070895
        }
      },
      "id": "2a12e51b-53bf-4ff0-998a-fb8017ed1e85"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (2.12.0)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (22.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (2.0.0)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: responses<0.19 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: multiprocess in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (0.14.1)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (6.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (9.0.0)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (2023.4.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: tzdata>=2022.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "78d8ba51-c5d8-4958-b9cf-1d36cf0b5500"
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install tensorflow_hub\n",
        "# %pip install tf_keras_vis"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584073681
        }
      },
      "id": "3c220cf2-5ae1-4f9e-98a3-f879334d89df"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584074243
        }
      },
      "id": "9651c12d-e510-4450-97d5-8f3a06c7ae3f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment configuration finished"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "6aa30d68-6cc4-408a-9b1e-6920a40a886b"
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1683343501848
        }
      },
      "id": "08f6df85"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import datasets"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1683584074773
        }
      },
      "id": "6f6c11bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {},
      "id": "b1a14cc5"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"train\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found cached dataset scientific_papers (/home/azureuser/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1683584076606
        }
      },
      "id": "97087531"
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n",
        "# val_dataset_full = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found cached dataset scientific_papers (/home/azureuser/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1683584077053
        }
      },
      "id": "33318445"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### show random downloaded text"
      ],
      "metadata": {},
      "id": "42121b27"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682374136183
        }
      },
      "id": "5955b545"
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_dataset)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Dataset({\n    features: ['article', 'abstract', 'section_names'],\n    num_rows: 119924\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1683343783911
        }
      },
      "id": "53a64822"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions we wrote are defined in oscar_utils.py for version control. Please DO NOT edit them on the fly here. any edits and addition to a Class or method defined must be added to oscar_utils.py"
      ],
      "metadata": {},
      "id": "db496145"
    },
    {
      "cell_type": "code",
      "source": [
        "# from oscar_utils import show_random_elements"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1683343784289
        }
      },
      "id": "7ab9c4c7"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_elements(dataset, num_examples=4):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "    return df\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1683343784679
        }
      },
      "id": "a4bb916a"
    },
    {
      "cell_type": "code",
      "source": [
        "random_df = show_random_elements(train_dataset, 1)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>abstract</th>\n      <th>section_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>vascular calcification , one of the major features in patients with chronic inflammatory disorders including type 2 diabetes mellitus , chronic kidney disease and atherosclerosis , is usually associated with significant adverse events and even mortality ( 1 , 2 ) .\\nvascular calcification is a complicated biological process which includes significant expression variations in alkaline phosphatase ( alp ) , osteocalcin ( oc ) , bone morphogenetic protein 2 ( bmp-2 ) and osteogenesis of transcription factor runx2 etc ( 2 - 5 ) .\\nmicrornas ( mirs ) are a large class of non - coding small rnas with 17 - 25 nucleotides ( 6 ) .\\nmirs are important regulators of gene expression on post - transcriptional level and participate in various normal physiological processes , whereas mir dysregulation could result in impaired cellular function and disease progression ( 7 ) . the associations of mirs with a variety of diseases have been reported , including cardio - vascular diseases , cancers and autoimmune diseases , however , the role of mirs in vascular calcification has been not extensively investigated and evidence for mirs modulation in vascular calcification is very limited ( 8 - 13 ) .\\ntill now , only a few mirs were identified to be associated with the pathogenesis of vascular calcification , such as mir-125b targeting sp7 and mir-204 targeting runx2 ( 14 , 15 ) . using vitamin d3 plus\\nnicotine induced rat aortic calcification model , we analyzed the mir expression profile in vascular calcification .\\nmoreover , our research also revealed for the first time that mir-297a was down - regulated in rats with vascular calcification , which consequently increased the level of its target fibroblast growth factor 23 ( fgf23 ) and enhanced calcification .\\nthe findings in our study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .\\nseven - week old , specific antigen free ( spf ) male sprague dawley ( sd ) rats weighing around 250 g were purchased from shanghai slack laboratory animal co , ltd and hosted in spf environment with food and water supplied .\\nall protocols involving animals were reviewed by the institutional ethical review board and performed with accordance to the provincial guidelines on animal experimentation .\\nrat vascular calcification model was built as previously described with modifications ( 16 , 17 ) . in brief , rats were received 300,000 iu / kg vitamin d3 ( sigma - aldrich ) once a day intramuscularly and 5 mg / kg nicotine ( sigma - aldrich ) twice a day orally for a continuous 4 weeks . for control rats , equal volumes of saline solution\\nthe body weight and blood pressure of each rat were measured at day 3 , 5 , 7 , 15 and 20 . at the end of week 4 , rats were anesthetized and blood samples were taken and sera were isolated and aliquoted and stored at -80 c .\\nthe levels of alp , phosphate and calcium in serum samples were measured using commercial colorimetric kits according to the manufacturer s instructions ( all kits were purchased from abcam , ab83369 for alp , ab102505 for calcium and ab65622 for phosphate respectively ) .\\ntotal rna was prepared using mirvana mirna isolation kit ( mirvana am1561 ) according the manufacturer s instructions and labeled with cy3 .\\nmirna chip was purchased from signosis ( ap-0003 ) and performed with accordance to the manufacturer s instructions .\\nmicroarray was scanned with genechipr scanner 3000 and data was analyzed with mirna qc tool software .\\ntotal rna was prepared as described in the mir chip assay . for detection of mir-297a\\nthe primers for mir-297a and internal control u6 amplification were purchased from ribobio inc . for detection of fgf23 and klotho expression ,\\nprimer pairs used in rt - pcr for , forward ; rev , reverse tissue was first homogenized on ice and centrifuged at 13000 g for 10 min at 4 c .\\nsuperna - tants were collected and protein concentration was determined using protein assay kit ( beyotime ) .\\nequal amount of samples were then isolated by 12% sds - page and transferred onto a 0.45 m pvdf membrane ( millipore ) .\\nsubsequently , membrane was first blocked with 5% non - fat milk for 1 hr at room temperature and then incubated with primary antibodies and corresponding hrp - conjugated secondary antibodies for 2 hr and 1hr at room temperature , respectively .\\nafter incubation , the membrane was extensively washed and immune - bands on the membrane were visualized using ecl substrate ( beyotime ) under a ccd camera ( alpha innotech ) .\\nthe gray - scale of the bands was analyzed by quantity - one v4.62 software .\\nthe relative expression of fgf23 and klotho was normalized to that of the internal control gapdh .\\nprimary antibodies anti - fgf23 , klotho and gapdh were all purchased from santa cruz .\\nall data were expressed as meanstandard deviation ( sd ) . for comparisons between two groups ,\\nseven - week old , specific antigen free ( spf ) male sprague dawley ( sd ) rats weighing around 250 g were purchased from shanghai slack laboratory animal co , ltd and hosted in spf environment with food and water supplied .\\nall protocols involving animals were reviewed by the institutional ethical review board and performed with accordance to the provincial guidelines on animal experimentation .\\nrat vascular calcification model was built as previously described with modifications ( 16 , 17 ) . in brief , rats were received 300,000 iu / kg vitamin d3 ( sigma - aldrich ) once a day intramuscularly and 5 mg / kg nicotine ( sigma - aldrich ) twice a day orally for a continuous 4 weeks . for control rats , equal volumes of saline solution\\nthe body weight and blood pressure of each rat were measured at day 3 , 5 , 7 , 15 and 20 . at the end of week 4 , rats were anesthetized and blood samples were taken and sera were isolated and aliquoted and stored at -80 c .\\nthe levels of alp , phosphate and calcium in serum samples were measured using commercial colorimetric kits according to the manufacturer s instructions ( all kits were purchased from abcam , ab83369 for alp , ab102505 for calcium and ab65622 for phosphate respectively ) .\\ntotal rna was prepared using mirvana mirna isolation kit ( mirvana am1561 ) according the manufacturer s instructions and labeled with cy3 .\\nmirna chip was purchased from signosis ( ap-0003 ) and performed with accordance to the manufacturer s instructions .\\nmicroarray was scanned with genechipr scanner 3000 and data was analyzed with mirna qc tool software .\\ntotal rna was prepared as described in the mir chip assay . for detection of mir-297a , stem - loop rt - pcr was performed .\\nthe primers for mir-297a and internal control u6 amplification were purchased from ribobio inc . for detection of fgf23 and klotho expression , regular rt - pcr was adopted and gapdh was used as an internal control .\\ntissue was first homogenized on ice and centrifuged at 13000 g for 10 min at 4 c .\\nsuperna - tants were collected and protein concentration was determined using protein assay kit ( beyotime ) .\\nequal amount of samples were then isolated by 12% sds - page and transferred onto a 0.45 m pvdf membrane ( millipore ) .\\nsubsequently , membrane was first blocked with 5% non - fat milk for 1 hr at room temperature and then incubated with primary antibodies and corresponding hrp - conjugated secondary antibodies for 2 hr and 1hr at room temperature , respectively . after incubation\\n, the membrane was extensively washed and immune - bands on the membrane were visualized using ecl substrate ( beyotime ) under a ccd camera ( alpha innotech ) .\\nthe gray - scale of the bands was analyzed by quantity - one v4.62 software .\\nthe relative expression of fgf23 and klotho was normalized to that of the internal control gapdh .\\nprimary antibodies anti - fgf23 , klotho and gapdh were all purchased from santa cruz .\\nall data were expressed as meanstandard deviation ( sd ) . for comparisons between two groups ,\\nas shown in figure 1 , vc rats demonstrated significantly slower weight gain comparing to control rats ( figure 1a ) .\\nmoreover , blood pressure of vc rats continuously increased as time increased , while that of control rats remained at a relatively constant level ( figure 1b ) .\\nmeasurement of calcium and phosphate concentrations as well as alp activities in serum samples 4 weeks after initial drug administration further revealed that these three elements in serum were also significantly increased ( figure 1c and d ) . at last ,\\na pathological assay ( von kossa staining ) was conducted to evaluate vc more directly . as shown in figure 1e ,\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\n( a and b ) the body weight ( a ) and blood pressure ( b ) were measured on day 3 , 5 , 7 , 15 and 20 .\\n( c and d ) four weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and concentrations of ca and phosphate ( c ) as well as alp activity ( d ) were measured .\\nrepresentative result is shown using mirna chip assay , we next analyzed the expression difference of mirnas between vc and control rats .\\nas shown in table 2 , our assay detected a total number of 16 mirs differently expressed in vc rats . among them , 10 mirs were up - regulated while the rest 6 were down - regulated .\\nof all the 16 mirs , mir-297a showed the sharpest expression change , consequently , this mir was chosen for more detailed mechanism analysis .\\nexpression profile difference of mirnas in vascular calcification rats first , we confirmed the mir279a expression level in vc and control rats using stem - loop rt - pcr and result was consistent with mir chip assay ( figure 2a ) .\\nnext , we analyzed the potential target gene of mir-297a using two programs pictar ( pictar.mdc-berlin.de/ ) and targetscan ( http://www.targetscan.org/mmu_61/ ) .\\nfgf23 is a member of the fibroblast growth factor family and participates in phosphate metabolism ( 18 ) . within the fgf23 signaling pathway ,\\nconsequently , we next analyzed the change of fgf23 and klotho in vc and control rats on both mrna and protein levels .\\nrt - pcr analysis revealed that the mrna level of fgf23 was significantly increased in vc rats while that of klotho was on the other hand decreased ( figure 2b ) .\\nthe measurement of fgf23 and klotho expression on protein level using western blot demonstrated similar tendencies ( figure 2c and d ) .\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\nfour weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and mir-297a was determined by stem - loop rt - pcr ( a ) .\\nthe expression of fgf23 and klotho were also determined by rt - pcr ( b ) and western blot ( c and d ) , respectively .\\n( d ) the gray scale of the immunobands was quantified and relative expression of fgf23 and klotho was calculated using gapdh as an internal control .\\n( a , b and d ) data shown are meansd of three independent experiments ( n=15 ) .\\ntaken together , our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 . the varied fgf23 and\\nits regulator klotho together might result in further enhancement on vascular calcification . over the past years\\nmirs have been discovered to be important regulators in both normal biological functions and various abnormalities .\\nmirs have been massively investigated in many diseases like cancers , and autoimmune diseases , but studies on their importance in vascular calcification are still very limited ( 14 , 15 ) . in the current study\\n, we identified 16 abnormally expressed mirs using a vascular calcification rat model . among these 16 mirs , 10 were up - regulated while the rest 6 were down - regulated .\\nour further study by focusing on one of down - regulated mir ( mir-297a ) revealed that it was positive regulator in vascular calcification by targeting fgf23 . in recent studies ,\\nmir-297a expression variation has also been described in many types of diseases including cerebral ischemia , s. japonicum infection and lung tumor ( 20 - 22 ) . however\\nklotho is an important protein in the fgf23 signaling pathway and the expression of fgf23 could affect the expression of klotho in a negative manner ( 24 ) . moreover ,\\nin the condition of klotho knock - out or suppression , fgf23 could enhance hyperphosphate - induced vc ( 25 , 26 ) . in our current study\\n, we revealed that mir-297a was down - regulated in vc , which consequently enhanced the expression of its target fgf23 .\\nthe abnormal up - regulation of fgf23 might then result in the decrease of klotho and vc .\\nof the 16 mirs identified in the current study , many have been reported to be participated in other diseases or normal biological processes .\\nfor instance , mir-126 involves in the development of mouse mammary gland and cardiac hypertrophy ( 27 , 28 ) , mir-23 in autoimmune inflammation and cancer metastasis ( 29 , 30 ) and mir-125b-5p in differential activation of macrophages and inflammation and cutaneous t cell lymphomas ( 31 , 32 ) .\\ngiven the complex of the mir regulation network , further research on whether and/or how these mirs participate in vascular calcification is warranted .\\ngiven that each disease has its own uniqueness in pathogenesis , the mechanism in triggering vascular calcification by different diseases might be distinctive . in the current study , all the experimentations were based on a rat vascular calcification model developed by vitamin d3 and nicotine administrations .\\ntherefore , whether this model is suitable for vascular calcification in all kinds of diseases remain to be further defined . taken together , our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 .\\nthe varied fgf23 and its regulator klotho together might result in further enhancement on vascular calcification .\\nthe findings in our study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .\\nas shown in figure 1 , vc rats demonstrated significantly slower weight gain comparing to control rats ( figure 1a ) .\\nmoreover , blood pressure of vc rats continuously increased as time increased , while that of control rats remained at a relatively constant level ( figure 1b ) .\\nmeasurement of calcium and phosphate concentrations as well as alp activities in serum samples 4 weeks after initial drug administration further revealed that these three elements in serum were also significantly increased ( figure 1c and d ) . at last ,\\na pathological assay ( von kossa staining ) was conducted to evaluate vc more directly . as shown in figure 1e ,\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\n( a and b ) the body weight ( a ) and blood pressure ( b ) were measured on day 3 , 5 , 7 , 15 and 20 .\\n( c and d ) four weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and concentrations of ca and phosphate ( c ) as well as alp activity ( d ) were measured .\\nusing mirna chip assay , we next analyzed the expression difference of mirnas between vc and control rats .\\nas shown in table 2 , our assay detected a total number of 16 mirs differently expressed in vc rats . among them , 10 mirs were up - regulated while the rest 6 were down - regulated .\\nof all the 16 mirs , mir-297a showed the sharpest expression change , consequently , this mir was chosen for more detailed mechanism analysis .\\nfirst , we confirmed the mir279a expression level in vc and control rats using stem - loop rt - pcr and result was consistent with mir chip assay ( figure 2a ) .\\nnext , we analyzed the potential target gene of mir-297a using two programs pictar ( pictar.mdc-berlin.de/ ) and targetscan ( http://www.targetscan.org/mmu_61/ ) .\\nfgf23 is a member of the fibroblast growth factor family and participates in phosphate metabolism ( 18 ) . within the fgf23 signaling pathway ,\\nconsequently , we next analyzed the change of fgf23 and klotho in vc and control rats on both mrna and protein levels .\\nrt - pcr analysis revealed that the mrna level of fgf23 was significantly increased in vc rats while that of klotho was on the other hand decreased ( figure 2b ) .\\nthe measurement of fgf23 and klotho expression on protein level using western blot demonstrated similar tendencies ( figure 2c and d ) .\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\nfour weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and mir-297a was determined by stem - loop rt - pcr ( a ) .\\nthe expression of fgf23 and klotho were also determined by rt - pcr ( b ) and western blot ( c and d ) , respectively .\\n( d ) the gray scale of the immunobands was quantified and relative expression of fgf23 and klotho was calculated using gapdh as an internal control .\\n( a , b and d ) data shown are meansd of three independent experiments ( n=15 ) .\\ntaken together , our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 .\\nthe varied fgf23 and its regulator klotho together might result in further enhancement on vascular calcification . over the past years\\nmirs have been discovered to be important regulators in both normal biological functions and various abnormalities .\\nmirs have been massively investigated in many diseases like cancers , and autoimmune diseases , but studies on their importance in vascular calcification are still very limited ( 14 , 15 ) . in the current study , we identified 16 abnormally expressed mirs using a vascular calcification rat model . among these 16 mirs ,\\nour further study by focusing on one of down - regulated mir ( mir-297a ) revealed that it was positive regulator in vascular calcification by targeting fgf23 . in recent studies ,\\nmir-297a expression variation has also been described in many types of diseases including cerebral ischemia , s. japonicum infection and lung tumor ( 20 - 22 ) .\\nklotho is an important protein in the fgf23 signaling pathway and the expression of fgf23 could affect the expression of klotho in a negative manner ( 24 ) . moreover ,\\nin the condition of klotho knock - out or suppression , fgf23 could enhance hyperphosphate - induced vc ( 25 , 26 ) . in our current study , we revealed that mir-297a was down - regulated in vc , which consequently enhanced the expression of its target fgf23 .\\nthe abnormal up - regulation of fgf23 might then result in the decrease of klotho and vc .\\nof the 16 mirs identified in the current study , many have been reported to be participated in other diseases or normal biological processes .\\nfor instance , mir-126 involves in the development of mouse mammary gland and cardiac hypertrophy ( 27 , 28 ) , mir-23 in autoimmune inflammation and cancer metastasis ( 29 , 30 ) and mir-125b-5p in differential activation of macrophages and inflammation and cutaneous t cell lymphomas ( 31 , 32 ) .\\ngiven the complex of the mir regulation network , further research on whether and/or how these mirs participate in vascular calcification is warranted .\\ngiven that each disease has its own uniqueness in pathogenesis , the mechanism in triggering vascular calcification by different diseases might be distinctive . in the current study ,\\nall the experimentations were based on a rat vascular calcification model developed by vitamin d3 and nicotine administrations .\\ntherefore , whether this model is suitable for vascular calcification in all kinds of diseases remain to be further defined . taken together\\n, our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 .\\nthe varied fgf23 and its regulator klotho together might result in further enhancement on vascular calcification .\\nthe findings in our study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .\\nour results indicated that fgf23 was target of mir-297a and decreased mir-297a in vascular calcification lead to the increase of fgf23 , which together with klotho might enhance vascular calcification .\\nthe findings of this study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .</td>\n      <td>objective(s):vascular calcification is one the major characteristics in patients with various types of chronic inflammatory disorders . \\n mirnas have been shown to be involved in many normal biological functions as well as diseases ; however , their role in vascular calcification has not received much attention.materials and methods : in the current study , we built a vascular calcification rat model using vitamin d3 plus nicotine and analyzed mirna expression profile by mirna chip assay . \\n potential target of one selected mirna with sharpest variation in expression were predicted by both pictar and targetscan . \\n the impact of the selected mirna on the expression of the potential target on both mrna and protein levels were measured by rt - pcr and western blot , respectively.results:our results identified 16 dysregulated mirnas , among which mir-297a showed the sharpest variation . \\n further analysis focusing on mir-297a revealed that fibroblast growth factor 23 ( fgf23 ) was a potential target of mir297a . \\n measurement of fgf23 and its regulator klotho on both mrna and protein levels demonstrated that fgf23 was significantly increased while klotho was decreased in rats with vascular calcification.conclusion:our results indicated that fgf23 was target of mir-297a and decreased mir-297a in vascular calcification lead to the increase of fgf23 , which together with klotho might enhance vascular calcification . \\n the findings of this study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .</td>\n      <td>Introduction\\nMaterials and Methods\\nAnimals and ethical statements\\nVascular calcification model construction and sampling\\nMeasurement of serum ALP, phosphate and calcium\\nMiR chip assay\\nRT-PCR\\nWestern blot\\nStatistical analysis\\nResults\\nConstruction of vascular calcification (VC) rat model\\nmiR expression profile difference between VC and control groups\\nDecreased miR-297a lead to FGF-23 expression increase in VC rats\\nConclusion</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1683343785094
        }
      },
      "id": "babab042"
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(random_df.to_html()))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>abstract</th>\n      <th>section_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>vascular calcification , one of the major features in patients with chronic inflammatory disorders including type 2 diabetes mellitus , chronic kidney disease and atherosclerosis , is usually associated with significant adverse events and even mortality ( 1 , 2 ) .\\nvascular calcification is a complicated biological process which includes significant expression variations in alkaline phosphatase ( alp ) , osteocalcin ( oc ) , bone morphogenetic protein 2 ( bmp-2 ) and osteogenesis of transcription factor runx2 etc ( 2 - 5 ) .\\nmicrornas ( mirs ) are a large class of non - coding small rnas with 17 - 25 nucleotides ( 6 ) .\\nmirs are important regulators of gene expression on post - transcriptional level and participate in various normal physiological processes , whereas mir dysregulation could result in impaired cellular function and disease progression ( 7 ) . the associations of mirs with a variety of diseases have been reported , including cardio - vascular diseases , cancers and autoimmune diseases , however , the role of mirs in vascular calcification has been not extensively investigated and evidence for mirs modulation in vascular calcification is very limited ( 8 - 13 ) .\\ntill now , only a few mirs were identified to be associated with the pathogenesis of vascular calcification , such as mir-125b targeting sp7 and mir-204 targeting runx2 ( 14 , 15 ) . using vitamin d3 plus\\nnicotine induced rat aortic calcification model , we analyzed the mir expression profile in vascular calcification .\\nmoreover , our research also revealed for the first time that mir-297a was down - regulated in rats with vascular calcification , which consequently increased the level of its target fibroblast growth factor 23 ( fgf23 ) and enhanced calcification .\\nthe findings in our study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .\\nseven - week old , specific antigen free ( spf ) male sprague dawley ( sd ) rats weighing around 250 g were purchased from shanghai slack laboratory animal co , ltd and hosted in spf environment with food and water supplied .\\nall protocols involving animals were reviewed by the institutional ethical review board and performed with accordance to the provincial guidelines on animal experimentation .\\nrat vascular calcification model was built as previously described with modifications ( 16 , 17 ) . in brief , rats were received 300,000 iu / kg vitamin d3 ( sigma - aldrich ) once a day intramuscularly and 5 mg / kg nicotine ( sigma - aldrich ) twice a day orally for a continuous 4 weeks . for control rats , equal volumes of saline solution\\nthe body weight and blood pressure of each rat were measured at day 3 , 5 , 7 , 15 and 20 . at the end of week 4 , rats were anesthetized and blood samples were taken and sera were isolated and aliquoted and stored at -80 c .\\nthe levels of alp , phosphate and calcium in serum samples were measured using commercial colorimetric kits according to the manufacturer s instructions ( all kits were purchased from abcam , ab83369 for alp , ab102505 for calcium and ab65622 for phosphate respectively ) .\\ntotal rna was prepared using mirvana mirna isolation kit ( mirvana am1561 ) according the manufacturer s instructions and labeled with cy3 .\\nmirna chip was purchased from signosis ( ap-0003 ) and performed with accordance to the manufacturer s instructions .\\nmicroarray was scanned with genechipr scanner 3000 and data was analyzed with mirna qc tool software .\\ntotal rna was prepared as described in the mir chip assay . for detection of mir-297a\\nthe primers for mir-297a and internal control u6 amplification were purchased from ribobio inc . for detection of fgf23 and klotho expression ,\\nprimer pairs used in rt - pcr for , forward ; rev , reverse tissue was first homogenized on ice and centrifuged at 13000 g for 10 min at 4 c .\\nsuperna - tants were collected and protein concentration was determined using protein assay kit ( beyotime ) .\\nequal amount of samples were then isolated by 12% sds - page and transferred onto a 0.45 m pvdf membrane ( millipore ) .\\nsubsequently , membrane was first blocked with 5% non - fat milk for 1 hr at room temperature and then incubated with primary antibodies and corresponding hrp - conjugated secondary antibodies for 2 hr and 1hr at room temperature , respectively .\\nafter incubation , the membrane was extensively washed and immune - bands on the membrane were visualized using ecl substrate ( beyotime ) under a ccd camera ( alpha innotech ) .\\nthe gray - scale of the bands was analyzed by quantity - one v4.62 software .\\nthe relative expression of fgf23 and klotho was normalized to that of the internal control gapdh .\\nprimary antibodies anti - fgf23 , klotho and gapdh were all purchased from santa cruz .\\nall data were expressed as meanstandard deviation ( sd ) . for comparisons between two groups ,\\nseven - week old , specific antigen free ( spf ) male sprague dawley ( sd ) rats weighing around 250 g were purchased from shanghai slack laboratory animal co , ltd and hosted in spf environment with food and water supplied .\\nall protocols involving animals were reviewed by the institutional ethical review board and performed with accordance to the provincial guidelines on animal experimentation .\\nrat vascular calcification model was built as previously described with modifications ( 16 , 17 ) . in brief , rats were received 300,000 iu / kg vitamin d3 ( sigma - aldrich ) once a day intramuscularly and 5 mg / kg nicotine ( sigma - aldrich ) twice a day orally for a continuous 4 weeks . for control rats , equal volumes of saline solution\\nthe body weight and blood pressure of each rat were measured at day 3 , 5 , 7 , 15 and 20 . at the end of week 4 , rats were anesthetized and blood samples were taken and sera were isolated and aliquoted and stored at -80 c .\\nthe levels of alp , phosphate and calcium in serum samples were measured using commercial colorimetric kits according to the manufacturer s instructions ( all kits were purchased from abcam , ab83369 for alp , ab102505 for calcium and ab65622 for phosphate respectively ) .\\ntotal rna was prepared using mirvana mirna isolation kit ( mirvana am1561 ) according the manufacturer s instructions and labeled with cy3 .\\nmirna chip was purchased from signosis ( ap-0003 ) and performed with accordance to the manufacturer s instructions .\\nmicroarray was scanned with genechipr scanner 3000 and data was analyzed with mirna qc tool software .\\ntotal rna was prepared as described in the mir chip assay . for detection of mir-297a , stem - loop rt - pcr was performed .\\nthe primers for mir-297a and internal control u6 amplification were purchased from ribobio inc . for detection of fgf23 and klotho expression , regular rt - pcr was adopted and gapdh was used as an internal control .\\ntissue was first homogenized on ice and centrifuged at 13000 g for 10 min at 4 c .\\nsuperna - tants were collected and protein concentration was determined using protein assay kit ( beyotime ) .\\nequal amount of samples were then isolated by 12% sds - page and transferred onto a 0.45 m pvdf membrane ( millipore ) .\\nsubsequently , membrane was first blocked with 5% non - fat milk for 1 hr at room temperature and then incubated with primary antibodies and corresponding hrp - conjugated secondary antibodies for 2 hr and 1hr at room temperature , respectively . after incubation\\n, the membrane was extensively washed and immune - bands on the membrane were visualized using ecl substrate ( beyotime ) under a ccd camera ( alpha innotech ) .\\nthe gray - scale of the bands was analyzed by quantity - one v4.62 software .\\nthe relative expression of fgf23 and klotho was normalized to that of the internal control gapdh .\\nprimary antibodies anti - fgf23 , klotho and gapdh were all purchased from santa cruz .\\nall data were expressed as meanstandard deviation ( sd ) . for comparisons between two groups ,\\nas shown in figure 1 , vc rats demonstrated significantly slower weight gain comparing to control rats ( figure 1a ) .\\nmoreover , blood pressure of vc rats continuously increased as time increased , while that of control rats remained at a relatively constant level ( figure 1b ) .\\nmeasurement of calcium and phosphate concentrations as well as alp activities in serum samples 4 weeks after initial drug administration further revealed that these three elements in serum were also significantly increased ( figure 1c and d ) . at last ,\\na pathological assay ( von kossa staining ) was conducted to evaluate vc more directly . as shown in figure 1e ,\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\n( a and b ) the body weight ( a ) and blood pressure ( b ) were measured on day 3 , 5 , 7 , 15 and 20 .\\n( c and d ) four weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and concentrations of ca and phosphate ( c ) as well as alp activity ( d ) were measured .\\nrepresentative result is shown using mirna chip assay , we next analyzed the expression difference of mirnas between vc and control rats .\\nas shown in table 2 , our assay detected a total number of 16 mirs differently expressed in vc rats . among them , 10 mirs were up - regulated while the rest 6 were down - regulated .\\nof all the 16 mirs , mir-297a showed the sharpest expression change , consequently , this mir was chosen for more detailed mechanism analysis .\\nexpression profile difference of mirnas in vascular calcification rats first , we confirmed the mir279a expression level in vc and control rats using stem - loop rt - pcr and result was consistent with mir chip assay ( figure 2a ) .\\nnext , we analyzed the potential target gene of mir-297a using two programs pictar ( pictar.mdc-berlin.de/ ) and targetscan ( http://www.targetscan.org/mmu_61/ ) .\\nfgf23 is a member of the fibroblast growth factor family and participates in phosphate metabolism ( 18 ) . within the fgf23 signaling pathway ,\\nconsequently , we next analyzed the change of fgf23 and klotho in vc and control rats on both mrna and protein levels .\\nrt - pcr analysis revealed that the mrna level of fgf23 was significantly increased in vc rats while that of klotho was on the other hand decreased ( figure 2b ) .\\nthe measurement of fgf23 and klotho expression on protein level using western blot demonstrated similar tendencies ( figure 2c and d ) .\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\nfour weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and mir-297a was determined by stem - loop rt - pcr ( a ) .\\nthe expression of fgf23 and klotho were also determined by rt - pcr ( b ) and western blot ( c and d ) , respectively .\\n( d ) the gray scale of the immunobands was quantified and relative expression of fgf23 and klotho was calculated using gapdh as an internal control .\\n( a , b and d ) data shown are meansd of three independent experiments ( n=15 ) .\\ntaken together , our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 . the varied fgf23 and\\nits regulator klotho together might result in further enhancement on vascular calcification . over the past years\\nmirs have been discovered to be important regulators in both normal biological functions and various abnormalities .\\nmirs have been massively investigated in many diseases like cancers , and autoimmune diseases , but studies on their importance in vascular calcification are still very limited ( 14 , 15 ) . in the current study\\n, we identified 16 abnormally expressed mirs using a vascular calcification rat model . among these 16 mirs , 10 were up - regulated while the rest 6 were down - regulated .\\nour further study by focusing on one of down - regulated mir ( mir-297a ) revealed that it was positive regulator in vascular calcification by targeting fgf23 . in recent studies ,\\nmir-297a expression variation has also been described in many types of diseases including cerebral ischemia , s. japonicum infection and lung tumor ( 20 - 22 ) . however\\nklotho is an important protein in the fgf23 signaling pathway and the expression of fgf23 could affect the expression of klotho in a negative manner ( 24 ) . moreover ,\\nin the condition of klotho knock - out or suppression , fgf23 could enhance hyperphosphate - induced vc ( 25 , 26 ) . in our current study\\n, we revealed that mir-297a was down - regulated in vc , which consequently enhanced the expression of its target fgf23 .\\nthe abnormal up - regulation of fgf23 might then result in the decrease of klotho and vc .\\nof the 16 mirs identified in the current study , many have been reported to be participated in other diseases or normal biological processes .\\nfor instance , mir-126 involves in the development of mouse mammary gland and cardiac hypertrophy ( 27 , 28 ) , mir-23 in autoimmune inflammation and cancer metastasis ( 29 , 30 ) and mir-125b-5p in differential activation of macrophages and inflammation and cutaneous t cell lymphomas ( 31 , 32 ) .\\ngiven the complex of the mir regulation network , further research on whether and/or how these mirs participate in vascular calcification is warranted .\\ngiven that each disease has its own uniqueness in pathogenesis , the mechanism in triggering vascular calcification by different diseases might be distinctive . in the current study , all the experimentations were based on a rat vascular calcification model developed by vitamin d3 and nicotine administrations .\\ntherefore , whether this model is suitable for vascular calcification in all kinds of diseases remain to be further defined . taken together , our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 .\\nthe varied fgf23 and its regulator klotho together might result in further enhancement on vascular calcification .\\nthe findings in our study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .\\nas shown in figure 1 , vc rats demonstrated significantly slower weight gain comparing to control rats ( figure 1a ) .\\nmoreover , blood pressure of vc rats continuously increased as time increased , while that of control rats remained at a relatively constant level ( figure 1b ) .\\nmeasurement of calcium and phosphate concentrations as well as alp activities in serum samples 4 weeks after initial drug administration further revealed that these three elements in serum were also significantly increased ( figure 1c and d ) . at last ,\\na pathological assay ( von kossa staining ) was conducted to evaluate vc more directly . as shown in figure 1e ,\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\n( a and b ) the body weight ( a ) and blood pressure ( b ) were measured on day 3 , 5 , 7 , 15 and 20 .\\n( c and d ) four weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and concentrations of ca and phosphate ( c ) as well as alp activity ( d ) were measured .\\nusing mirna chip assay , we next analyzed the expression difference of mirnas between vc and control rats .\\nas shown in table 2 , our assay detected a total number of 16 mirs differently expressed in vc rats . among them , 10 mirs were up - regulated while the rest 6 were down - regulated .\\nof all the 16 mirs , mir-297a showed the sharpest expression change , consequently , this mir was chosen for more detailed mechanism analysis .\\nfirst , we confirmed the mir279a expression level in vc and control rats using stem - loop rt - pcr and result was consistent with mir chip assay ( figure 2a ) .\\nnext , we analyzed the potential target gene of mir-297a using two programs pictar ( pictar.mdc-berlin.de/ ) and targetscan ( http://www.targetscan.org/mmu_61/ ) .\\nfgf23 is a member of the fibroblast growth factor family and participates in phosphate metabolism ( 18 ) . within the fgf23 signaling pathway ,\\nconsequently , we next analyzed the change of fgf23 and klotho in vc and control rats on both mrna and protein levels .\\nrt - pcr analysis revealed that the mrna level of fgf23 was significantly increased in vc rats while that of klotho was on the other hand decreased ( figure 2b ) .\\nthe measurement of fgf23 and klotho expression on protein level using western blot demonstrated similar tendencies ( figure 2c and d ) .\\nrats were received vitamin d3 and nicotine ( vc ) or saline solution ( control ) daily for 4 weeks .\\nfour weeks after the initial administration of vitamin d3 and nicotine , rats were sacrificed and mir-297a was determined by stem - loop rt - pcr ( a ) .\\nthe expression of fgf23 and klotho were also determined by rt - pcr ( b ) and western blot ( c and d ) , respectively .\\n( d ) the gray scale of the immunobands was quantified and relative expression of fgf23 and klotho was calculated using gapdh as an internal control .\\n( a , b and d ) data shown are meansd of three independent experiments ( n=15 ) .\\ntaken together , our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 .\\nthe varied fgf23 and its regulator klotho together might result in further enhancement on vascular calcification . over the past years\\nmirs have been discovered to be important regulators in both normal biological functions and various abnormalities .\\nmirs have been massively investigated in many diseases like cancers , and autoimmune diseases , but studies on their importance in vascular calcification are still very limited ( 14 , 15 ) . in the current study , we identified 16 abnormally expressed mirs using a vascular calcification rat model . among these 16 mirs ,\\nour further study by focusing on one of down - regulated mir ( mir-297a ) revealed that it was positive regulator in vascular calcification by targeting fgf23 . in recent studies ,\\nmir-297a expression variation has also been described in many types of diseases including cerebral ischemia , s. japonicum infection and lung tumor ( 20 - 22 ) .\\nklotho is an important protein in the fgf23 signaling pathway and the expression of fgf23 could affect the expression of klotho in a negative manner ( 24 ) . moreover ,\\nin the condition of klotho knock - out or suppression , fgf23 could enhance hyperphosphate - induced vc ( 25 , 26 ) . in our current study , we revealed that mir-297a was down - regulated in vc , which consequently enhanced the expression of its target fgf23 .\\nthe abnormal up - regulation of fgf23 might then result in the decrease of klotho and vc .\\nof the 16 mirs identified in the current study , many have been reported to be participated in other diseases or normal biological processes .\\nfor instance , mir-126 involves in the development of mouse mammary gland and cardiac hypertrophy ( 27 , 28 ) , mir-23 in autoimmune inflammation and cancer metastasis ( 29 , 30 ) and mir-125b-5p in differential activation of macrophages and inflammation and cutaneous t cell lymphomas ( 31 , 32 ) .\\ngiven the complex of the mir regulation network , further research on whether and/or how these mirs participate in vascular calcification is warranted .\\ngiven that each disease has its own uniqueness in pathogenesis , the mechanism in triggering vascular calcification by different diseases might be distinctive . in the current study ,\\nall the experimentations were based on a rat vascular calcification model developed by vitamin d3 and nicotine administrations .\\ntherefore , whether this model is suitable for vascular calcification in all kinds of diseases remain to be further defined . taken together\\n, our results here indicated that mir-297a was decreased in vc rats , which consequently increased the expression of its regulation target fgf23 .\\nthe varied fgf23 and its regulator klotho together might result in further enhancement on vascular calcification .\\nthe findings in our study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .\\nour results indicated that fgf23 was target of mir-297a and decreased mir-297a in vascular calcification lead to the increase of fgf23 , which together with klotho might enhance vascular calcification .\\nthe findings of this study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .</td>\n      <td>objective(s):vascular calcification is one the major characteristics in patients with various types of chronic inflammatory disorders . \\n mirnas have been shown to be involved in many normal biological functions as well as diseases ; however , their role in vascular calcification has not received much attention.materials and methods : in the current study , we built a vascular calcification rat model using vitamin d3 plus nicotine and analyzed mirna expression profile by mirna chip assay . \\n potential target of one selected mirna with sharpest variation in expression were predicted by both pictar and targetscan . \\n the impact of the selected mirna on the expression of the potential target on both mrna and protein levels were measured by rt - pcr and western blot , respectively.results:our results identified 16 dysregulated mirnas , among which mir-297a showed the sharpest variation . \\n further analysis focusing on mir-297a revealed that fibroblast growth factor 23 ( fgf23 ) was a potential target of mir297a . \\n measurement of fgf23 and its regulator klotho on both mrna and protein levels demonstrated that fgf23 was significantly increased while klotho was decreased in rats with vascular calcification.conclusion:our results indicated that fgf23 was target of mir-297a and decreased mir-297a in vascular calcification lead to the increase of fgf23 , which together with klotho might enhance vascular calcification . \\n the findings of this study could provide valuable information for the understanding of mechanisms underlying mir - dependent vascular calcification as well as potential treatment target for the disease .</td>\n      <td>Introduction\\nMaterials and Methods\\nAnimals and ethical statements\\nVascular calcification model construction and sampling\\nMeasurement of serum ALP, phosphate and calcium\\nMiR chip assay\\nRT-PCR\\nWestern blot\\nStatistical analysis\\nResults\\nConstruction of vascular calcification (VC) rat model\\nmiR expression profile difference between VC and control groups\\nDecreased miR-297a lead to FGF-23 expression increase in VC rats\\nConclusion</td>\n    </tr>\n  </tbody>\n</table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1683343785539
        }
      },
      "id": "ed1be9d6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import autotokenizer, from long-text transformer on huggingface"
      ],
      "metadata": {},
      "id": "f36c031f"
    },
    {
      "cell_type": "code",
      "source": [
        "# for EDA we use  \"smaller\" LED checkpoint \"allenai/led-base-16384\". Better performance can however be attained by finetuning \"allenai/led-large-16384\" at the cost of a higher required GPU RAM."
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1683343785895
        }
      },
      "id": "d73359d0"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1683584078965
        }
      },
      "id": "7af69e18"
    },
    {
      "cell_type": "code",
      "source": [
        "#we start with longformer: LongformerTokenizer (AllenAI Longformer model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1683584079422
        }
      },
      "id": "6a831eb8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pubmed's input data has a median token length of 2715 with the 90%-ile token length being 6101. The output data has a media token length of 171 with the 90%-ile token length being 352.\n",
        "(cited from Big Bird: Transformers for Longer Sequences paper : https://arxiv.org/pdf/2007.14062.pdf)\n",
        "\n",
        "Thus, we set the maximum input length to 8192 and the maximum output length to 512 to ensure that the model can attend to almost all input tokens is able to generate up to a large enough number of output tokens.\n",
        "\n",
        "In this notebook, we are only able to train on batch_size=2 to prevent out-of-memory errors."
      ],
      "metadata": {},
      "id": "31f5d833"
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 8192\n",
        "max_output_length = 512\n",
        "batch_size = 2"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1683343790242
        }
      },
      "id": "5fc2ce85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format. As explained earlier article represents here our input data and abstract is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
        "\n",
        "In addition to the usual attention_mask, LED can make use of an additional global_attention_mask defining which input tokens are attended globally and which are attended only locally, just as it's the case of Longformer. For more information on Longformer's self-attention, please take a look at the corresponding docs. For summarization, we follow recommendations of the paper and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to -100."
      ],
      "metadata": {},
      "id": "bc86e40c"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "    # tokenize the inputs and labels\n",
        "    #Returns a dictionary containing the encoded sequence or sequence pair \n",
        "    # and additional information: the mask for sequence classification and \n",
        "    # the overflowing elements if a max_length is specified.\n",
        "    inputs = tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        batch[\"abstract\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_output_length,\n",
        "    )\n",
        "\n",
        "    #Hugingface tokenizer returns the attention mask and input ids.\n",
        "    #     input_ids: list of token ids to be fed to a model\n",
        "    # token_type_ids: list of token type ids to be fed to a model\n",
        "    # attention_mask: list of indices specifying which tokens should be attended to by the model\n",
        "    # overflowing_tokens: list of overflowing tokens sequences if a max length is specified and return_overflowing_tokens=True.\n",
        "    # special_tokens_mask: if adding special tokens, this is a list of [0, 1], with 0 specifying special added tokens and 1 specifying sequence tokens.\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    # len(batch[\"input_ids\"]) = minibatch size ( or batch_size)\n",
        "    # len(batch[\"input_ids\"][0]) = max_input_length\n",
        "    # in effect a 2-d Python List of dimension (batch_size x max_input_length)    \n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored. current pad_token_id is = 1 in this model\n",
        "    # batch labels is the output tokenizer dimension (batch_size x max_output_length)\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1683343790626
        }
      },
      "id": "a19855f3"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "e015e67a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downsample for EDA and development!"
      ],
      "metadata": {},
      "id": "8511d432"
    },
    {
      "cell_type": "code",
      "source": [
        "DOWNSAMPLE_COUNT = 150\n",
        "train_dataset = train_dataset.select(range(DOWNSAMPLE_COUNT))\n",
        "val_dataset = val_dataset.select(range(DOWNSAMPLE_COUNT))"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1683343791028
        }
      },
      "id": "39695dda"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/150 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "571ae395d7bc4a489c3dabd84cf7267d"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1683343793486
        }
      },
      "id": "2bfbc703"
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = val_dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/150 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed7dffec9f4344478f64c83c724d9548"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1683343795579
        }
      },
      "id": "f73027d1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset to Torch format"
      ],
      "metadata": {},
      "id": "c7b57a73"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")\n",
        "val_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1683343799475
        }
      },
      "id": "8fee9683"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model via the AutoModelForSeq2SeqLM class."
      ],
      "metadata": {},
      "id": "c04ae011"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1683584079875
        }
      },
      "id": "ea8afbd9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tradeoff just in EDA: We've decided to stick to the smaller model \"allenai/led-base-16384\" for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."
      ],
      "metadata": {},
      "id": "2417d87c"
    },
    {
      "cell_type": "code",
      "source": [
        "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\",\n",
        "                                            gradient_checkpointing=True,\n",
        "                                            use_cache=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/648M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "356805011f83445b9121b8be6eeaf0e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cc3c43924e54e69afa81d4ce667ba0a"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1683343811006
        }
      },
      "id": "245d09c7"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "f049e0d3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training: we need to evaluate the model on Rouge\n",
        "\n",
        "ROUGE score: https://torchmetrics.readthedocs.io/en/stable/text/rouge_score.html\n",
        "Per Wikipedia: Recall-Oriented Understudy for Gisting Evaluation. It's a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n",
        "\n",
        "ROUGE is currently the most well used summarization metrics (as proposed in Milestone #1) that we will use.\n",
        "\n",
        "Setting fitting generation parameters for loss calculation.\n",
        "\n",
        "- beam search with a small beam of just 2 to save memory. \n",
        "- force the model to generate at least 100 tokens, but no more than 512.\n",
        "- setting earlyt stopping and no_repeat_ngram to prevent spurious repetition"
      ],
      "metadata": {},
      "id": "05528cb3"
    },
    {
      "cell_type": "code",
      "source": [
        "# set generate hyperparameters\n",
        "led.config.num_beams = 2\n",
        "led.config.max_length = 512\n",
        "led.config.min_length = 100\n",
        "led.config.length_penalty = 2.0\n",
        "led.config.early_stopping = True\n",
        "led.config.no_repeat_ngram_size = 3"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1683343811383
        }
      },
      "id": "a45c8f0a"
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = load_metric(\"rouge\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_6041/4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  rouge = load_metric(\"rouge\")\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3240fb9895c7431fb7c72ca62af8067b"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1683343814790
        }
      },
      "id": "8e8024d1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compute metrics function expects the generated output = pred.predictions\n",
        "and true label = pred.label_ids.\n",
        "\n",
        "We need to decode the tokens then calculate ROUGE"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "aff7673c"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    # true labels\n",
        "    labels_ids = pred.label_ids\n",
        "    # predicted labels\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # decode in batch from tokens into string both true string and predicted string\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    # compute rouge score\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1683343815181
        }
      },
      "id": "1ad7b7d4"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1683343815572
        }
      },
      "id": "86b3e05b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2SeqTrainer : we eventually want to turn on predict_with_generate=True to inspect. Right now I'm turning it off to save RAM.\n",
        "\n",
        "Current batch size is set to 2, then I do gradient_accumulation_steps=4 to batch the gradient, to conserve RAM. Effective batch size becomes (batch_size = 2) * (grad_accum_steps = 4) = 8"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "656a6b5f-d25f-4229-b9a2-d35647b201db"
    },
    {
      "cell_type": "code",
      "source": [
        "#MKIMMINS: IF CUDA\n",
        "# enabled floating point precision : fp16 apex training\n",
        "\n",
        "#IF NO CUDA ON MPI, DO NOT USE FP16\n",
        "CUDA = False\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=CUDA,\n",
        "    output_dir=\"./\",\n",
        "    logging_steps=5,\n",
        "    eval_steps=10,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683343815972
        }
      },
      "id": "fe9822ba-a138-4b28-9a73-63ff51addf95"
    },
    {
      "cell_type": "code",
      "source": [
        "#MKIMMINS VAL DATASET TURNED OFF FOR SPEED\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=led,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset\n",
        "    ,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683343816487
        }
      },
      "id": "a9a7e2af-1a57-42a5-a755-741d7ea65c6f"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683316681771
        }
      },
      "id": "4b66a434-f9d4-4657-b6c9-b344050070bb"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "a1f48a02-6a46-405f-a198-a68f070a3115"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a LEDTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='11' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/18 59:58 < 46:38, 0.00 it/s, Epoch 0.53/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='24' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24/75 9:11:13 < 20:22:16, 0.00 it/s]\n    </div>\n    "
          },
          "metadata": {}
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683343816860
        }
      },
      "id": "7b168ca8-ba45-4c0b-bcd9-fc5805b4e26e"
    },
    {
      "cell_type": "code",
      "source": [
        "# led.save(\"mkimmins_fine_tune_10\")\n",
        "import time\n",
        "yyyymmddhhmm = time.strftime(\"%Y%m%d%H%M\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683345167977
        }
      },
      "id": "c90e5482-2717-4234-b00c-295881de6eda"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"saving model to\")\n",
        "print(f\"./led_roberta_save{yyyymmddhhmm}\")\n",
        "trainer.save_model(f\"./led_roberta_save{yyyymmddhhmm}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683345280589
        }
      },
      "id": "c4fbe68d-94a6-4158-a647-e1f93e0fb7fc"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "PATH = './led_roberta_save202305062343'\n",
        "\n",
        "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(PATH, local_files_only=True)"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584340831
        }
      },
      "id": "92984924-6238-45c3-bd1f-2feb38a2547e"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as torch"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584095423
        }
      },
      "id": "0c5b0689-52bd-4db4-a0ac-e715a394dec3"
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "LEDForConditionalGeneration(\n  (led): LEDModel(\n    (shared): Embedding(50265, 768, padding_idx=1)\n    (encoder): LEDEncoder(\n      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n      (layers): ModuleList(\n        (0): LEDEncoderLayer(\n          (self_attn): LEDEncoderAttention(\n            (longformer_self_attn): LEDEncoderSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (query_global): Linear(in_features=768, out_features=768, bias=True)\n              (key_global): Linear(in_features=768, out_features=768, bias=True)\n              (value_global): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): LEDEncoderLayer(\n          (self_attn): LEDEncoderAttention(\n            (longformer_self_attn): LEDEncoderSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (query_global): Linear(in_features=768, out_features=768, bias=True)\n              (key_global): Linear(in_features=768, out_features=768, bias=True)\n              (value_global): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): LEDEncoderLayer(\n          (self_attn): LEDEncoderAttention(\n            (longformer_self_attn): LEDEncoderSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (query_global): Linear(in_features=768, out_features=768, bias=True)\n              (key_global): Linear(in_features=768, out_features=768, bias=True)\n              (value_global): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): LEDEncoderLayer(\n          (self_attn): LEDEncoderAttention(\n            (longformer_self_attn): LEDEncoderSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (query_global): Linear(in_features=768, out_features=768, bias=True)\n              (key_global): Linear(in_features=768, out_features=768, bias=True)\n              (value_global): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): LEDEncoderLayer(\n          (self_attn): LEDEncoderAttention(\n            (longformer_self_attn): LEDEncoderSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (query_global): Linear(in_features=768, out_features=768, bias=True)\n              (key_global): Linear(in_features=768, out_features=768, bias=True)\n              (value_global): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): LEDEncoderLayer(\n          (self_attn): LEDEncoderAttention(\n            (longformer_self_attn): LEDEncoderSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (query_global): Linear(in_features=768, out_features=768, bias=True)\n              (key_global): Linear(in_features=768, out_features=768, bias=True)\n              (value_global): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): LEDDecoder(\n      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n      (layers): ModuleList(\n        (0): LEDDecoderLayer(\n          (self_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): LEDDecoderLayer(\n          (self_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): LEDDecoderLayer(\n          (self_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): LEDDecoderLayer(\n          (self_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): LEDDecoderLayer(\n          (self_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): LEDDecoderLayer(\n          (self_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): LEDDecoderAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584341331
        }
      },
      "id": "fb125799-a724-467b-92a1-e18d235bdd97"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e3b25e76-a7b6-4d11-aa5e-0c72ff188f56"
    },
    {
      "cell_type": "code",
      "source": [
        "# load pubmed\n",
        "pubmed_test = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"test\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def generate_answer(batch):\n",
        "  \n",
        "  inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=8192, return_tensors=\"pt\", truncation=True)\n",
        "  input_ids = inputs_dict.input_ids.to(device)\n",
        "  attention_mask = inputs_dict.attention_mask.to(device)\n",
        "  global_attention_mask = torch.zeros_like(attention_mask)\n",
        "  # put global attention on <s> token\n",
        "  global_attention_mask[:, 0] = 1\n",
        "\n",
        "  predicted_abstract_ids = loaded_model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
        "  batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
        "  return batch\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/datasets/load.py:1748: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\nYou can remove this warning by passing 'verification_mode=no_checks' instead.\n  warnings.warn(\nFound cached dataset scientific_papers (/home/azureuser/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1683584294902
        }
      },
      "id": "a4dedef4"
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "'cpu'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584297281
        }
      },
      "id": "20a149b4-cfa5-4b9a-98a9-f1eb52c9eeaf"
    },
    {
      "cell_type": "code",
      "source": [
        "len(pubmed_test)\n",
        "DOWNSAMPLE_COUNT = 20\n",
        "pubmed_test_downsample = pubmed_test.select(range(DOWNSAMPLE_COUNT))\n"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584301107
        }
      },
      "id": "57bb5fd4-0dd6-4017-bbb1-ea7c920a58d4"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = pubmed_test_downsample.map(generate_answer, batched=True, batch_size=4)\n",
        "\n",
        "# load rouge\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/20 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57e4f617d02148529b14cee7ee0c6b67"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683584455060
        }
      },
      "id": "695759b7-292d-4d4f-9b23-0605be58ce5d"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "32010c4e-4474-4038-9ee4-b45ea3a94341"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9b3872f1-72bc-43a8-b2a2-3cfd40543b83"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b129708d-a919-4be2-b1e5-01b10c9ba59b"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "273f2e5c-77f0-4fc4-ae8c-beb80d3dc033"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}