{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install pip install transformers\n",
        "!pip install tiktoken\n",
        "!pip install wandb\n",
        "!pip install tqdm\n",
        "!unzip nanoGPT.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfTEYRUfakTg",
        "outputId": "3dbf9b2a-e462-4df7-ec46-49b78b8a4571"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.0.1)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, install, transformers\n",
            "Successfully installed install-1.3.5 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=18ee7d5ad15f7dc157ca6f31d55980c173f3107e0bf2ca7dfd6b72ec2f7518de\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.22.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Archive:  nanoGPT.zip\n",
            "   creating: nanoGPT/\n",
            "   creating: nanoGPT/.git/\n",
            "  inflating: nanoGPT/.gitattributes  \n",
            " extracting: nanoGPT/.gitignore      \n",
            "  inflating: nanoGPT/.git/config     \n",
            "  inflating: nanoGPT/.git/description  \n",
            "  inflating: nanoGPT/.git/FETCH_HEAD  \n",
            " extracting: nanoGPT/.git/HEAD       \n",
            "   creating: nanoGPT/.git/hooks/\n",
            "  inflating: nanoGPT/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: nanoGPT/.git/hooks/commit-msg.sample  \n",
            "  inflating: nanoGPT/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: nanoGPT/.git/hooks/post-update.sample  \n",
            "  inflating: nanoGPT/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: nanoGPT/.git/hooks/pre-commit.sample  \n",
            "  inflating: nanoGPT/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: nanoGPT/.git/hooks/pre-push.sample  \n",
            "  inflating: nanoGPT/.git/hooks/pre-rebase.sample  \n",
            "  inflating: nanoGPT/.git/hooks/pre-receive.sample  \n",
            "  inflating: nanoGPT/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: nanoGPT/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: nanoGPT/.git/hooks/update.sample  \n",
            "  inflating: nanoGPT/.git/index      \n",
            "   creating: nanoGPT/.git/info/\n",
            "  inflating: nanoGPT/.git/info/exclude  \n",
            "   creating: nanoGPT/.git/logs/\n",
            "  inflating: nanoGPT/.git/logs/HEAD  \n",
            "   creating: nanoGPT/.git/logs/refs/\n",
            "   creating: nanoGPT/.git/logs/refs/heads/\n",
            "  inflating: nanoGPT/.git/logs/refs/heads/master  \n",
            "   creating: nanoGPT/.git/logs/refs/remotes/\n",
            "   creating: nanoGPT/.git/logs/refs/remotes/origin/\n",
            "  inflating: nanoGPT/.git/logs/refs/remotes/origin/HEAD  \n",
            "   creating: nanoGPT/.git/objects/\n",
            "   creating: nanoGPT/.git/objects/info/\n",
            "   creating: nanoGPT/.git/objects/pack/\n",
            "  inflating: nanoGPT/.git/objects/pack/pack-abf1b9b81f868196665e0f77c8ac56e7bff7cf7d.idx  \n",
            "  inflating: nanoGPT/.git/objects/pack/pack-abf1b9b81f868196665e0f77c8ac56e7bff7cf7d.pack  \n",
            "  inflating: nanoGPT/.git/packed-refs  \n",
            "   creating: nanoGPT/.git/refs/\n",
            "   creating: nanoGPT/.git/refs/heads/\n",
            " extracting: nanoGPT/.git/refs/heads/master  \n",
            "   creating: nanoGPT/.git/refs/remotes/\n",
            "   creating: nanoGPT/.git/refs/remotes/origin/\n",
            " extracting: nanoGPT/.git/refs/remotes/origin/HEAD  \n",
            "   creating: nanoGPT/.git/refs/tags/\n",
            "   creating: nanoGPT/assets/\n",
            "  inflating: nanoGPT/assets/gpt2_124M_loss.png  \n",
            "  inflating: nanoGPT/assets/nanogpt.jpg  \n",
            "  inflating: nanoGPT/bench.py        \n",
            "   creating: nanoGPT/config/\n",
            "  inflating: nanoGPT/configurator.py  \n",
            "  inflating: nanoGPT/config/eval_gpt2.py  \n",
            "  inflating: nanoGPT/config/eval_gpt2_large.py  \n",
            "  inflating: nanoGPT/config/eval_gpt2_medium.py  \n",
            "  inflating: nanoGPT/config/eval_gpt2_xl.py  \n",
            "  inflating: nanoGPT/config/finetune_shakespeare.py  \n",
            "  inflating: nanoGPT/config/train_gpt2.py  \n",
            "  inflating: nanoGPT/config/train_shakespeare_char.py  \n",
            "   creating: nanoGPT/data/\n",
            "   creating: nanoGPT/data/openwebtext/\n",
            "  inflating: nanoGPT/data/openwebtext/prepare.py  \n",
            "  inflating: nanoGPT/data/openwebtext/readme.md  \n",
            "   creating: nanoGPT/data/shakespeare/\n",
            "  inflating: nanoGPT/data/shakespeare/prepare.py  \n",
            "  inflating: nanoGPT/data/shakespeare/readme.md  \n",
            "   creating: nanoGPT/data/shakespeare_char/\n",
            "  inflating: nanoGPT/data/shakespeare_char/prepare.py  \n",
            "  inflating: nanoGPT/data/shakespeare_char/readme.md  \n",
            "  inflating: nanoGPT/LICENSE         \n",
            "  inflating: nanoGPT/model.py        \n",
            "  inflating: nanoGPT/README.md       \n",
            "  inflating: nanoGPT/sample.py       \n",
            "  inflating: nanoGPT/scaling_laws.ipynb  \n",
            "  inflating: nanoGPT/train.py        \n",
            "  inflating: nanoGPT/transformer_sizing.ipynb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import random\n",
        "import pandas as pd\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "5RnAbnPQbNBH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T971gSCakQjM",
        "outputId": "51c7b243-0548-40d4-a0f5-24ceeb7c9eee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ_nf58ckW4E",
        "outputId": "df874c97-f5f2-4e8f-f4ff-70686146203f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "[2023-05-08 18:35:06,167] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:06,709] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:07,554] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:07,887] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:08,335] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:08,670] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:09,121] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:09,458] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:09,910] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:10,247] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:10,869] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-08 18:35:11,200] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2716, time 23734.97ms, mfu -100.00%\n",
            "iter 10: loss 3.2437, time 24.53ms, mfu 15.19%\n",
            "iter 20: loss 2.7944, time 25.22ms, mfu 15.15%\n",
            "iter 30: loss 2.6423, time 24.60ms, mfu 15.15%\n",
            "iter 40: loss 2.5811, time 25.12ms, mfu 15.11%\n",
            "iter 50: loss 2.5297, time 24.98ms, mfu 15.09%\n",
            "iter 60: loss 2.5140, time 24.41ms, mfu 15.11%\n",
            "iter 70: loss 2.4999, time 24.09ms, mfu 15.15%\n",
            "iter 80: loss 2.4943, time 24.34ms, mfu 15.16%\n",
            "iter 90: loss 2.4628, time 24.33ms, mfu 15.18%\n",
            "iter 100: loss 2.4589, time 25.22ms, mfu 15.14%\n",
            "iter 110: loss 2.4542, time 24.51ms, mfu 15.14%\n",
            "iter 120: loss 2.4329, time 24.83ms, mfu 15.13%\n",
            "iter 130: loss 2.4120, time 24.76ms, mfu 15.12%\n",
            "iter 140: loss 2.4011, time 26.01ms, mfu 15.04%\n",
            "iter 150: loss 2.4126, time 27.93ms, mfu 14.87%\n",
            "iter 160: loss 2.3796, time 24.64ms, mfu 14.90%\n",
            "iter 170: loss 2.3509, time 25.70ms, mfu 14.86%\n",
            "iter 180: loss 2.3079, time 24.89ms, mfu 14.87%\n",
            "iter 190: loss 2.2540, time 25.95ms, mfu 14.82%\n",
            "iter 200: loss 2.2186, time 24.66ms, mfu 14.85%\n",
            "iter 210: loss 2.1464, time 25.43ms, mfu 14.83%\n",
            "iter 220: loss 2.1460, time 26.44ms, mfu 14.75%\n",
            "iter 230: loss 2.0743, time 24.37ms, mfu 14.81%\n",
            "iter 240: loss 2.0872, time 24.46ms, mfu 14.85%\n",
            "step 250: train loss 1.9722, val loss 2.0745\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0399, time 6502.65ms, mfu 13.37%\n",
            "iter 260: loss 1.9927, time 25.18ms, mfu 13.51%\n",
            "iter 270: loss 1.9731, time 24.46ms, mfu 13.69%\n",
            "iter 280: loss 2.0010, time 24.98ms, mfu 13.81%\n",
            "iter 290: loss 1.9215, time 24.33ms, mfu 13.96%\n",
            "iter 300: loss 1.9164, time 24.76ms, mfu 14.07%\n",
            "iter 310: loss 1.8751, time 24.44ms, mfu 14.19%\n",
            "iter 320: loss 1.8569, time 24.89ms, mfu 14.26%\n",
            "iter 330: loss 1.8294, time 24.15ms, mfu 14.38%\n",
            "iter 340: loss 1.7909, time 24.32ms, mfu 14.48%\n",
            "iter 350: loss 1.8390, time 28.11ms, mfu 14.35%\n",
            "iter 360: loss 1.7809, time 24.75ms, mfu 14.42%\n",
            "iter 370: loss 1.7519, time 24.30ms, mfu 14.51%\n",
            "iter 380: loss 1.7329, time 24.51ms, mfu 14.58%\n",
            "iter 390: loss 1.7410, time 24.23ms, mfu 14.66%\n",
            "iter 400: loss 1.7707, time 24.20ms, mfu 14.74%\n",
            "iter 410: loss 1.7040, time 24.30ms, mfu 14.80%\n",
            "iter 420: loss 1.7239, time 24.55ms, mfu 14.83%\n",
            "iter 430: loss 1.6888, time 25.09ms, mfu 14.84%\n",
            "iter 440: loss 1.6606, time 24.52ms, mfu 14.87%\n",
            "iter 450: loss 1.6606, time 28.35ms, mfu 14.70%\n",
            "iter 460: loss 1.6062, time 25.86ms, mfu 14.67%\n",
            "iter 470: loss 1.6686, time 24.37ms, mfu 14.73%\n",
            "iter 480: loss 1.6209, time 25.09ms, mfu 14.74%\n",
            "iter 490: loss 1.5985, time 24.77ms, mfu 14.77%\n",
            "step 500: train loss 1.5310, val loss 1.7356\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6063, time 3403.95ms, mfu 13.31%\n",
            "iter 510: loss 1.6221, time 24.96ms, mfu 13.47%\n",
            "iter 520: loss 1.5964, time 24.49ms, mfu 13.64%\n",
            "iter 530: loss 1.5642, time 24.98ms, mfu 13.77%\n",
            "iter 540: loss 1.6279, time 24.40ms, mfu 13.92%\n",
            "iter 550: loss 1.5669, time 25.59ms, mfu 13.99%\n",
            "iter 560: loss 1.5716, time 24.69ms, mfu 14.10%\n",
            "iter 570: loss 1.5707, time 24.85ms, mfu 14.19%\n",
            "iter 580: loss 1.5446, time 24.31ms, mfu 14.30%\n",
            "iter 590: loss 1.4997, time 24.60ms, mfu 14.38%\n",
            "iter 600: loss 1.5145, time 25.05ms, mfu 14.43%\n",
            "iter 610: loss 1.5437, time 25.22ms, mfu 14.47%\n",
            "iter 620: loss 1.5314, time 24.75ms, mfu 14.53%\n",
            "iter 630: loss 1.5168, time 24.92ms, mfu 14.57%\n",
            "iter 640: loss 1.4681, time 24.83ms, mfu 14.61%\n",
            "iter 650: loss 1.5022, time 24.34ms, mfu 14.68%\n",
            "iter 660: loss 1.5083, time 24.53ms, mfu 14.73%\n",
            "iter 670: loss 1.4514, time 24.70ms, mfu 14.77%\n",
            "iter 680: loss 1.5116, time 24.34ms, mfu 14.82%\n",
            "iter 690: loss 1.4705, time 24.61ms, mfu 14.85%\n",
            "iter 700: loss 1.4852, time 24.66ms, mfu 14.88%\n",
            "iter 710: loss 1.4541, time 24.48ms, mfu 14.91%\n",
            "iter 720: loss 1.4415, time 24.44ms, mfu 14.95%\n",
            "iter 730: loss 1.4237, time 24.59ms, mfu 14.97%\n",
            "iter 740: loss 1.4338, time 24.35ms, mfu 15.00%\n",
            "step 750: train loss 1.3661, val loss 1.5950\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4269, time 3409.61ms, mfu 13.51%\n",
            "iter 760: loss 1.4552, time 24.87ms, mfu 13.66%\n",
            "iter 770: loss 1.4276, time 24.66ms, mfu 13.80%\n",
            "iter 780: loss 1.4231, time 24.09ms, mfu 13.97%\n",
            "iter 790: loss 1.4207, time 24.16ms, mfu 14.12%\n",
            "iter 800: loss 1.4358, time 25.16ms, mfu 14.19%\n",
            "iter 810: loss 1.4089, time 24.71ms, mfu 14.27%\n",
            "iter 820: loss 1.4047, time 25.01ms, mfu 14.34%\n",
            "iter 830: loss 1.3918, time 24.58ms, mfu 14.42%\n",
            "iter 840: loss 1.4063, time 24.56ms, mfu 14.49%\n",
            "iter 850: loss 1.3941, time 24.75ms, mfu 14.55%\n",
            "iter 860: loss 1.3986, time 26.02ms, mfu 14.53%\n",
            "iter 870: loss 1.3968, time 24.85ms, mfu 14.57%\n",
            "iter 880: loss 1.3807, time 24.91ms, mfu 14.61%\n",
            "iter 890: loss 1.3861, time 25.20ms, mfu 14.63%\n",
            "iter 900: loss 1.3708, time 25.26ms, mfu 14.64%\n",
            "iter 910: loss 1.3254, time 25.20ms, mfu 14.66%\n",
            "iter 920: loss 1.3614, time 25.05ms, mfu 14.68%\n",
            "iter 930: loss 1.3579, time 24.64ms, mfu 14.72%\n",
            "iter 940: loss 1.3466, time 25.99ms, mfu 14.68%\n",
            "iter 950: loss 1.3609, time 24.56ms, mfu 14.73%\n",
            "iter 960: loss 1.3590, time 24.66ms, mfu 14.77%\n",
            "iter 970: loss 1.3642, time 26.01ms, mfu 14.73%\n",
            "iter 980: loss 1.3539, time 24.73ms, mfu 14.76%\n",
            "iter 990: loss 1.3380, time 24.99ms, mfu 14.78%\n",
            "step 1000: train loss 1.2784, val loss 1.5296\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3364, time 3399.36ms, mfu 13.31%\n",
            "iter 1010: loss 1.3319, time 24.38ms, mfu 13.51%\n",
            "iter 1020: loss 1.3165, time 24.62ms, mfu 13.67%\n",
            "iter 1030: loss 1.3314, time 24.81ms, mfu 13.80%\n",
            "iter 1040: loss 1.3568, time 24.47ms, mfu 13.95%\n",
            "iter 1050: loss 1.2917, time 25.18ms, mfu 14.03%\n",
            "iter 1060: loss 1.3411, time 24.77ms, mfu 14.13%\n",
            "iter 1070: loss 1.3380, time 24.40ms, mfu 14.25%\n",
            "iter 1080: loss 1.3284, time 24.44ms, mfu 14.35%\n",
            "iter 1090: loss 1.3494, time 24.27ms, mfu 14.45%\n",
            "iter 1100: loss 1.3243, time 24.31ms, mfu 14.54%\n",
            "iter 1110: loss 1.2962, time 24.59ms, mfu 14.60%\n",
            "iter 1120: loss 1.3008, time 24.42ms, mfu 14.66%\n",
            "iter 1130: loss 1.3029, time 25.24ms, mfu 14.67%\n",
            "iter 1140: loss 1.2966, time 24.52ms, mfu 14.73%\n",
            "iter 1150: loss 1.3057, time 24.77ms, mfu 14.76%\n",
            "iter 1160: loss 1.3243, time 25.31ms, mfu 14.75%\n",
            "iter 1170: loss 1.2964, time 24.72ms, mfu 14.79%\n",
            "iter 1180: loss 1.3151, time 24.65ms, mfu 14.82%\n",
            "iter 1190: loss 1.2746, time 24.39ms, mfu 14.86%\n",
            "iter 1200: loss 1.2963, time 24.80ms, mfu 14.88%\n",
            "iter 1210: loss 1.2643, time 25.57ms, mfu 14.85%\n",
            "iter 1220: loss 1.3019, time 24.00ms, mfu 14.92%\n",
            "iter 1230: loss 1.2982, time 24.49ms, mfu 14.95%\n",
            "iter 1240: loss 1.2995, time 25.07ms, mfu 14.94%\n",
            "step 1250: train loss 1.2053, val loss 1.5014\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2758, time 3450.28ms, mfu 13.46%\n",
            "iter 1260: loss 1.2829, time 24.57ms, mfu 13.63%\n",
            "iter 1270: loss 1.2620, time 24.13ms, mfu 13.81%\n",
            "iter 1280: loss 1.2572, time 24.07ms, mfu 13.98%\n",
            "iter 1290: loss 1.2838, time 24.84ms, mfu 14.08%\n",
            "iter 1300: loss 1.3015, time 24.44ms, mfu 14.19%\n",
            "iter 1310: loss 1.2397, time 24.46ms, mfu 14.30%\n",
            "iter 1320: loss 1.3088, time 24.30ms, mfu 14.40%\n",
            "iter 1330: loss 1.2577, time 24.67ms, mfu 14.47%\n",
            "iter 1340: loss 1.2942, time 24.40ms, mfu 14.55%\n",
            "iter 1350: loss 1.2542, time 24.49ms, mfu 14.62%\n",
            "iter 1360: loss 1.2741, time 24.71ms, mfu 14.66%\n",
            "iter 1370: loss 1.2569, time 24.30ms, mfu 14.73%\n",
            "iter 1380: loss 1.2702, time 24.70ms, mfu 14.77%\n",
            "iter 1390: loss 1.2468, time 27.79ms, mfu 14.63%\n",
            "iter 1400: loss 1.2604, time 24.86ms, mfu 14.67%\n",
            "iter 1410: loss 1.2479, time 24.60ms, mfu 14.72%\n",
            "iter 1420: loss 1.2744, time 24.98ms, mfu 14.74%\n",
            "iter 1430: loss 1.2425, time 24.81ms, mfu 14.76%\n",
            "iter 1440: loss 1.2472, time 25.42ms, mfu 14.75%\n",
            "iter 1450: loss 1.2311, time 24.53ms, mfu 14.80%\n",
            "iter 1460: loss 1.2478, time 24.24ms, mfu 14.85%\n",
            "iter 1470: loss 1.2168, time 25.05ms, mfu 14.86%\n",
            "iter 1480: loss 1.2114, time 24.48ms, mfu 14.89%\n",
            "iter 1490: loss 1.2351, time 24.68ms, mfu 14.91%\n",
            "step 1500: train loss 1.1541, val loss 1.4861\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1853, time 3399.79ms, mfu 13.43%\n",
            "iter 1510: loss 1.2348, time 24.80ms, mfu 13.59%\n",
            "iter 1520: loss 1.2324, time 24.87ms, mfu 13.73%\n",
            "iter 1530: loss 1.2551, time 24.82ms, mfu 13.86%\n",
            "iter 1540: loss 1.1956, time 25.02ms, mfu 13.96%\n",
            "iter 1550: loss 1.2344, time 24.61ms, mfu 14.08%\n",
            "iter 1560: loss 1.2059, time 25.28ms, mfu 14.15%\n",
            "iter 1570: loss 1.2353, time 24.46ms, mfu 14.26%\n",
            "iter 1580: loss 1.2047, time 25.36ms, mfu 14.30%\n",
            "iter 1590: loss 1.1840, time 25.58ms, mfu 14.33%\n",
            "iter 1600: loss 1.1946, time 25.24ms, mfu 14.37%\n",
            "iter 1610: loss 1.2350, time 24.76ms, mfu 14.44%\n",
            "iter 1620: loss 1.1844, time 25.01ms, mfu 14.48%\n",
            "iter 1630: loss 1.2042, time 24.49ms, mfu 14.56%\n",
            "iter 1640: loss 1.2011, time 24.80ms, mfu 14.60%\n",
            "iter 1650: loss 1.1873, time 24.73ms, mfu 14.65%\n",
            "iter 1660: loss 1.2186, time 26.18ms, mfu 14.61%\n",
            "iter 1670: loss 1.1944, time 24.87ms, mfu 14.65%\n",
            "iter 1680: loss 1.1975, time 24.50ms, mfu 14.70%\n",
            "iter 1690: loss 1.2046, time 25.08ms, mfu 14.72%\n",
            "iter 1700: loss 1.1849, time 25.55ms, mfu 14.70%\n",
            "iter 1710: loss 1.1784, time 24.63ms, mfu 14.75%\n",
            "iter 1720: loss 1.1783, time 24.48ms, mfu 14.79%\n",
            "iter 1730: loss 1.1980, time 24.56ms, mfu 14.83%\n",
            "iter 1740: loss 1.1640, time 25.28ms, mfu 14.82%\n",
            "step 1750: train loss 1.1035, val loss 1.4795\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1925, time 3419.17ms, mfu 13.35%\n",
            "iter 1760: loss 1.1792, time 24.34ms, mfu 13.55%\n",
            "iter 1770: loss 1.2011, time 24.62ms, mfu 13.71%\n",
            "iter 1780: loss 1.1947, time 24.49ms, mfu 13.86%\n",
            "iter 1790: loss 1.1927, time 24.73ms, mfu 13.98%\n",
            "iter 1800: loss 1.1864, time 24.21ms, mfu 14.12%\n",
            "iter 1810: loss 1.1624, time 24.86ms, mfu 14.21%\n",
            "iter 1820: loss 1.1697, time 24.42ms, mfu 14.31%\n",
            "iter 1830: loss 1.1713, time 24.59ms, mfu 14.40%\n",
            "iter 1840: loss 1.1564, time 24.12ms, mfu 14.50%\n",
            "iter 1850: loss 1.1621, time 24.35ms, mfu 14.58%\n",
            "Process ForkProcess-12:\n",
            "Process ForkProcess-11:\n",
            "Process ForkProcess-9:\n",
            "Process ForkProcess-10:\n",
            "Process ForkProcess-7:\n",
            "Process ForkProcess-8:\n",
            "Process ForkProcess-5:\n",
            "Process ForkProcess-6:\n",
            "Process ForkProcess-4:\n",
            "Process ForkProcess-2:\n",
            "Process ForkProcess-3:\n",
            "Process ForkProcess-1:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqyS6Q_Wn-O4",
        "outputId": "75b055bc-0342-4e17-b4f4-d7146f14f51d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And cowards it be but back'd by your hands;\n",
            "And what hands are the purpose to be determineed?\n",
            "\n",
            "ISABELLA:\n",
            "Even your mother, and full of men! What like eye\n",
            "Have been mildly made overthrow me now?\n",
            "\n",
            "ANGELO:\n",
            "I know not, and thus: but if the princess, why he's\n",
            "not known to live your highness than\n",
            "A man of your pity sound more rivers and merry so more for us\n",
            "To be thrusted so; so you must not know I this fault?\n",
            "\n",
            "ISABELLA:\n",
            "I shall too fish a day of into the hoice of a\n",
            "craver-breath-changed vill\n",
            "---------------\n",
            "\n",
            "Men part, and grave with the easts of the sea-sense;\n",
            "But when that she was contented with one.\n",
            "\n",
            "JULIET:\n",
            "I'll stay you to the case of the joy.\n",
            "\n",
            "LADY CAPULET:\n",
            "I think it strength in me; that's still the rest.\n",
            "\n",
            "JULIET:\n",
            "Sir, I shall not show it.\n",
            "It is not the heavier and beggar,--\n",
            "\n",
            "Nurse:\n",
            "To make unlook'd upon the shore.\n",
            "\n",
            "Nurse:\n",
            "Living, if I should be brief, be some more that it is\n",
            "often nor the submission; and is better it, both\n",
            "speakling kind for him, the issue with this\n",
            "of the gloric; you have me\n",
            "---------------\n",
            "\n",
            "Men take it with all, I will not go with me.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Thanks, ay, my lord, my lords: thinks I will not see,\n",
            "I would to my tongue did cheque to my father:\n",
            "AI, then all in the day, the anger king,\n",
            "To menarch infect him of witness.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Sister, ay, in his shame likelihood.\n",
            "The old conscience of my lord, unto the limbs,\n",
            "Of his fair state, whose destroyalty parts have been sound,\n",
            "And then I have most bloed wind more him.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Be you that growing you of Richard holds,\n",
            "Sh\n",
            "---------------\n",
            "\n",
            "The sext of Romeo, that have said as I hate to\n",
            "As virtue as but the seemed, there would\n",
            "Be reported with prevail'd with you.\n",
            "\n",
            "MENENIUS:\n",
            "'Tis deserve my lord\n",
            "In the arms, and in his love him, his as hour\n",
            "\n",
            "CORIOLANUS:\n",
            "Now her does, her wife is but as that as I did,\n",
            "Have I made to our gentleman with swords. To all to our wounds\n",
            "What is the world I would did supply call upon:\n",
            "And so, sir, thy proud head, and sent my aged bosom,\n",
            "And so inform and the brain shall I find thee such more.\n",
            "\n",
            "RICHARD:\n",
            "\n",
            "Mess\n",
            "---------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sample.py\", line 87, in <module>\n",
            "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/model.py\", line 323, in generate\n",
            "    logits, _ = self(idx_cond)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/model.py\", line 188, in forward\n",
            "    x = block(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/model.py\", line 112, in forward\n",
            "    x = x + self.mlp(self.ln_2(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/model.py\", line 96, in forward\n",
            "    x = new_gelu(x)\n",
            "  File \"/content/model.py\", line 24, in new_gelu\n",
            "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sample.py\", line 85, in <module>\n",
            "    with ctx:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 291, in __exit__\n",
            "    torch.clear_autocast_cache()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/pubmed/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8HPxKm1vOx_",
        "outputId": "bb4f2ca0-c23e-4be5-ac56-53636c1533f1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found cached dataset scientific_papers (/content/drive/MyDrive/109b project/dataset_cache/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n",
            "writing /content/data/pubmed/train.bin: 100% 1024/1024 [00:08<00:00, 126.20it/s]\n",
            "writing /content/data/pubmed/val.bin:   6% 60/1024 [00:00<00:01, 550.53it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/data/pubmed/prepare.py\", line 52, in <module>\n",
            "    batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 4583, in shard\n",
            "    return self.select(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 543, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/fingerprint.py\", line 511, in wrapper\n",
            "    out = func(dataset, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3771, in select\n",
            "    return self._select_contiguous(start, length, new_fingerprint=new_fingerprint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 543, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/fingerprint.py\", line 511, in wrapper\n",
            "    out = func(dataset, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3831, in _select_contiguous\n",
            "    _check_valid_indices_value(start, len(self))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 635, in _check_valid_indices_value\n",
            "    raise IndexError(f\"Index {index} out of range for dataset of size {size}.\")\n",
            "IndexError: Index 60 out of range for dataset of size 60.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/finetune_pubmed.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjwgM3asxAGS",
        "outputId": "36acff5c-ac1d-4bb2-e4a7-742a7d0af92e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_pubmed.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-pubmed'\n",
            "# eval stuff\n",
            "eval_interval = 100\n",
            "eval_iters = 20\n",
            "log_interval = 10\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'pubmed'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'pubmed'\n",
            "init_from = 'gpt2-large' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 6\n",
            "gradient_accumulation_steps = 5 * 8\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "max_iters = 6000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokens per iteration will be: 245,760\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-large\n",
            "loading weights from pretrained gpt: gpt2-large\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 772.72M\n",
            "Downloading (…)lve/main/config.json: 100% 666/666 [00:00<00:00, 2.45MB/s]\n",
            "Downloading pytorch_model.bin: 100% 3.25G/3.25G [00:06<00:00, 502MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 711kB/s]\n",
            "num decayed parameter tensors: 146, with 773,428,480 parameters\n",
            "num non-decayed parameter tensors: 290, with 601,600 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 2.9866, val loss 2.9948\n",
            "iter 0: loss 2.6513, time 98277.57ms, mfu -100.00%\n",
            "iter 10: loss 2.6916, time 10845.06ms, mfu 37.79%\n",
            "iter 20: loss 2.6063, time 10858.46ms, mfu 37.78%\n",
            "iter 30: loss 2.7115, time 10861.64ms, mfu 37.78%\n",
            "iter 40: loss 2.6132, time 10859.72ms, mfu 37.77%\n",
            "iter 50: loss 2.3274, time 10861.85ms, mfu 37.77%\n",
            "iter 60: loss 2.4177, time 10864.35ms, mfu 37.76%\n",
            "iter 70: loss 2.4461, time 10864.20ms, mfu 37.76%\n",
            "iter 80: loss 2.4047, time 11613.97ms, mfu 37.51%\n",
            "iter 90: loss 2.3863, time 10863.97ms, mfu 37.53%\n",
            "step 100: train loss 2.4942, val loss 2.4637\n",
            "saving checkpoint to out-pubmed\n",
            "iter 100: loss 2.2747, time 41565.16ms, mfu 34.77%\n",
            "iter 110: loss 2.4968, time 10857.52ms, mfu 35.06%\n",
            "iter 120: loss 2.8483, time 10865.13ms, mfu 35.33%\n",
            "iter 130: loss 2.5968, time 10864.46ms, mfu 35.57%\n",
            "iter 140: loss 2.5043, time 10860.97ms, mfu 35.78%\n",
            "iter 150: loss 2.4457, time 10865.47ms, mfu 35.98%\n",
            "iter 160: loss 2.4664, time 10866.21ms, mfu 36.15%\n",
            "iter 170: loss 2.5689, time 10864.89ms, mfu 36.31%\n",
            "iter 180: loss 2.3778, time 10864.49ms, mfu 36.45%\n",
            "iter 190: loss 2.4574, time 10867.28ms, mfu 36.57%\n",
            "step 200: train loss 2.4342, val loss 2.4703\n",
            "iter 200: loss 2.4501, time 13951.13ms, mfu 35.85%\n",
            "iter 210: loss 2.2145, time 10865.05ms, mfu 36.04%\n",
            "iter 220: loss 2.5390, time 10864.35ms, mfu 36.21%\n",
            "iter 230: loss 2.4046, time 10860.56ms, mfu 36.36%\n",
            "iter 240: loss 2.4209, time 10866.41ms, mfu 36.50%\n",
            "iter 250: loss 1.9208, time 10864.56ms, mfu 36.62%\n",
            "iter 260: loss 2.5948, time 10863.93ms, mfu 36.73%\n",
            "iter 270: loss 2.5993, time 10862.67ms, mfu 36.83%\n",
            "iter 280: loss 2.3227, time 10863.55ms, mfu 36.92%\n",
            "iter 290: loss 2.4003, time 10863.98ms, mfu 37.00%\n",
            "step 300: train loss 2.3509, val loss 2.4695\n",
            "iter 300: loss 2.5846, time 13944.55ms, mfu 36.24%\n",
            "iter 310: loss 2.3696, time 10860.98ms, mfu 36.39%\n",
            "iter 320: loss 2.4438, time 10864.36ms, mfu 36.52%\n",
            "iter 330: loss 2.1744, time 10862.14ms, mfu 36.64%\n",
            "iter 340: loss 2.4854, time 10861.18ms, mfu 36.75%\n",
            "iter 350: loss 2.3269, time 10859.10ms, mfu 36.85%\n",
            "iter 360: loss 2.3110, time 10861.33ms, mfu 36.94%\n",
            "iter 370: loss 2.4568, time 10862.66ms, mfu 37.02%\n",
            "iter 380: loss 2.3813, time 10861.90ms, mfu 37.09%\n",
            "iter 390: loss 2.3886, time 10864.40ms, mfu 37.15%\n",
            "step 400: train loss 2.3262, val loss 2.4224\n",
            "saving checkpoint to out-pubmed\n",
            "iter 400: loss 2.3752, time 41579.62ms, mfu 34.42%\n",
            "iter 410: loss 2.1829, time 10861.25ms, mfu 34.75%\n",
            "iter 420: loss 2.4030, time 10865.20ms, mfu 35.05%\n",
            "iter 430: loss 2.5477, time 10862.01ms, mfu 35.32%\n",
            "iter 440: loss 2.5460, time 10862.06ms, mfu 35.56%\n",
            "iter 450: loss 2.3547, time 11203.85ms, mfu 35.66%\n",
            "iter 460: loss 2.2380, time 10864.16ms, mfu 35.87%\n",
            "iter 470: loss 2.4094, time 10863.65ms, mfu 36.05%\n",
            "iter 480: loss 2.4123, time 10865.22ms, mfu 36.22%\n",
            "iter 490: loss 2.1674, time 10867.73ms, mfu 36.37%\n",
            "step 500: train loss 2.3651, val loss 2.2706\n",
            "saving checkpoint to out-pubmed\n",
            "iter 500: loss 2.2608, time 41107.65ms, mfu 33.73%\n",
            "iter 510: loss 2.4664, time 10860.96ms, mfu 34.13%\n",
            "iter 520: loss 2.2022, time 10861.15ms, mfu 34.49%\n",
            "iter 530: loss 1.9819, time 10869.12ms, mfu 34.81%\n",
            "iter 540: loss 2.2418, time 10864.91ms, mfu 35.10%\n",
            "iter 550: loss 2.6253, time 10866.94ms, mfu 35.36%\n",
            "iter 560: loss 2.6564, time 10865.48ms, mfu 35.60%\n",
            "iter 570: loss 2.2035, time 10863.21ms, mfu 35.81%\n",
            "iter 580: loss 2.4493, time 10867.37ms, mfu 36.00%\n",
            "iter 590: loss 2.3644, time 10866.27ms, mfu 36.17%\n",
            "step 600: train loss 2.3362, val loss 2.3800\n",
            "iter 600: loss 2.1901, time 13942.46ms, mfu 35.49%\n",
            "iter 610: loss 2.1186, time 10870.58ms, mfu 35.71%\n",
            "iter 620: loss 2.3525, time 10868.22ms, mfu 35.91%\n",
            "iter 630: loss 2.6908, time 10870.97ms, mfu 36.09%\n",
            "iter 640: loss 2.5290, time 10868.33ms, mfu 36.25%\n",
            "iter 650: loss 2.2571, time 10868.05ms, mfu 36.40%\n",
            "iter 660: loss 2.3622, time 10867.27ms, mfu 36.53%\n",
            "iter 670: loss 2.5649, time 10865.82ms, mfu 36.65%\n",
            "iter 680: loss 2.4537, time 10863.99ms, mfu 36.76%\n",
            "iter 690: loss 2.2922, time 10864.00ms, mfu 36.85%\n",
            "step 700: train loss 2.2807, val loss 2.3759\n",
            "iter 700: loss 2.4655, time 13941.53ms, mfu 36.11%\n",
            "iter 710: loss 2.1774, time 10867.79ms, mfu 36.27%\n",
            "iter 720: loss 2.3054, time 10863.28ms, mfu 36.41%\n",
            "iter 730: loss 2.2800, time 10863.89ms, mfu 36.54%\n",
            "iter 740: loss 2.0692, time 10865.31ms, mfu 36.66%\n",
            "iter 750: loss 2.4857, time 10862.63ms, mfu 36.77%\n",
            "iter 760: loss 2.4254, time 10865.61ms, mfu 36.86%\n",
            "iter 770: loss 2.1134, time 10863.28ms, mfu 36.95%\n",
            "iter 780: loss 2.4579, time 10862.68ms, mfu 37.03%\n",
            "iter 790: loss 1.9912, time 10865.88ms, mfu 37.09%\n",
            "step 800: train loss 2.2826, val loss 2.4001\n",
            "iter 800: loss 1.9629, time 13938.25ms, mfu 36.33%\n",
            "iter 810: loss 2.0884, time 10864.13ms, mfu 36.46%\n",
            "iter 820: loss 2.3875, time 10866.54ms, mfu 36.59%\n",
            "iter 830: loss 2.5657, time 10866.84ms, mfu 36.70%\n",
            "iter 840: loss 2.4718, time 10865.56ms, mfu 36.80%\n",
            "iter 850: loss 2.3801, time 10866.56ms, mfu 36.89%\n",
            "iter 860: loss 2.4653, time 10863.92ms, mfu 36.98%\n",
            "iter 870: loss 2.5554, time 10865.70ms, mfu 37.05%\n",
            "iter 880: loss 2.3562, time 10861.97ms, mfu 37.12%\n",
            "iter 890: loss 2.1178, time 10865.87ms, mfu 37.18%\n",
            "step 900: train loss 2.3629, val loss 2.3549\n",
            "iter 900: loss 2.4471, time 13936.88ms, mfu 36.40%\n",
            "iter 910: loss 2.1669, time 10865.32ms, mfu 36.53%\n",
            "iter 920: loss 2.3115, time 10865.96ms, mfu 36.65%\n",
            "iter 930: loss 2.1099, time 10870.98ms, mfu 36.75%\n",
            "iter 940: loss 2.2923, time 10865.69ms, mfu 36.85%\n",
            "iter 950: loss 2.1393, time 10862.44ms, mfu 36.94%\n",
            "iter 960: loss 1.9724, time 10866.82ms, mfu 37.02%\n",
            "iter 970: loss 2.3648, time 10864.59ms, mfu 37.09%\n",
            "Process ForkProcess-11:\n",
            "Process ForkProcess-7:\n",
            "Process ForkProcess-10:\n",
            "Process ForkProcess-5:\n",
            "Process ForkProcess-6:\n",
            "Process ForkProcess-8:\n",
            "Process ForkProcess-9:\n",
            "Process ForkProcess-12:\n",
            "Process ForkProcess-4:\n",
            "Process ForkProcess-1:\n",
            "Process ForkProcess-3:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Process ForkProcess-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 300, in <module>\n",
            "    scaler.scale(loss).backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['abstract']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yntfXQ4b_Pfo",
        "outputId": "95ea3b26-ded8-4805-cc53-dc2af955ddd3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran . \\n the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention . for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) . \\n however , there were no significant changes among boys or total population . \\n the mean of all anthropometric indices changed significantly after intervention both among girls and boys as well as in total population . \\n the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001).conclusion : this study demonstrates the potential success and scalability of school feeding programs in iran . \\n community nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children . \"]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One nice feature of this is that we can specify the length of the article or abstract we want to generate from the text.\n",
        "\n",
        "# Should make a wrapper that makes it easier to input article, section and abstract. \n",
        "\n",
        "# A short article ends up look a lot like an abstract"
      ],
      "metadata": {
        "id": "MxMF4UU08P_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-pubmed --start=\"ARTICLE STARTS HERE:\" --num_samples=2 --max_new_tokens=1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvNFf6Wn4L0m",
        "outputId": "dcc68be0-bcfe-47c4-b788-6e2dd6978848"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-pubmed\n",
            "Overriding: start = ARTICLE STARTS HERE:\n",
            "Overriding: num_samples = 2\n",
            "Overriding: max_new_tokens = 1000\n",
            "number of parameters: 772.72M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "ARTICLE STARTS HERE:dysphagia is a common and severe problem in patients with type 1 diabetes . in general ,\n",
            "dysphagia increases the risk of developing severe hypoglycemic episodes , but there is no clear relationship between glucose and dysphagia .\n",
            "a study published in 2003 showed that the occurrence of dysphagia was higher in individuals with type 1 diabetes than in those without type 1 diabetes .\n",
            "dysphagia results from a combination of protein production defects , hypolactatemia , hypoglycemic attacks , and electrolyte abnormalities .\n",
            "dysphagia can be prevented by eating food that is rich in fiber and by taking dietary supplements or by adjusting the amount of physical activity .\n",
            "however , studies on the effect of dietary supplements on glucose levels have not been performed . in this study , in this cohort of patients with type 1 diabetes , we aimed to evaluate the effect of dietary supplements on the occurrence of hypoglycemic episodes and to investigate the association between different dietary supplements and hypoglycemia .\n",
            "this is a cross - sectional study conducted at the diabetes center of the kuwait royal health system ( krhs ) in jeddah , saudi arabia during january 2012 to september 2013 .\n",
            "a total of 205 patients with type 1 diabetes and who visited the krhs with the diagnosis of type 1 diabetes were recruited .\n",
            "the patients were identified according to the krhs criteria for inclusion of type 1 diabetes .\n",
            "the inclusion criteria were as follows : ( i ) age younger than 18 years and ( ii ) lack of hypoglycemic episodes for at least 12 months .\n",
            "all patients were recruited in the outpatient department of the krhs from january 2012 to september 2013 .\n",
            "the exclusion criteria were as follows : ( i ) patients with diabetic ketoacidosis , ( ii ) patients with comorbidities , such as diabetes and hypertension , ( iii ) patients with severe congenital disorders , such as congenital heart disease , rheumatoid arthritis , or endocrine disorders ( such as thyroid disorders or hypothyroidism ) , ( iv ) patients with severe malnutrition , ( v ) patients who were not able to write the medical history and ( vi ) patients with eating disorder .\n",
            "the patients were required to sign the informed consent form in order to participate in the study and to take part in the study .\n",
            "the patients were provided with a dietary supplement and information about the dosages of the dietary supplements .\n",
            "all patients were asked to follow a high - fiber ( 20% to 24% of total calories ) diet for 6 months .\n",
            "the subjects were also asked to take part in the study by taking part of the dietary supplement .\n",
            "dietary supplements applied in the present study were as follows : taurine , gentamicin , tiamet herb , ferrous sulfate , aloe vera , pectin , niacin , and vitamin b2 .\n",
            "the dietary supplements were provided by the commercial company lc biowaste , jeddah , saudi arabia . two patients started taking dietary supplements .\n",
            "the dietary supplement was distributed to the subjects one week before their visit to the clinic .\n",
            "dietary supplements were administered to the patients in a 1:1 ratio and patients were instructed to take these dietary supplements every day for one month .\n",
            "the subjects were instructed to follow the daily recommendations for fasting blood sugar ( fbs ) measurement , and to take their blood sugar measurements from the time of administration of the supplement to their blood sugar measurement .\n",
            "the blood samples were collected at the same time of day and throughout the duration of the period of study , and the fbs results were obtained within two hours after the administration of the dietary supplement .\n",
            "the subjects were instructed to take the dietary supplements at bedtime to avoid a possible hypoglycemia .\n",
            "the blood samples were collected in the morning after the subjects fell asleep .\n",
            "the fbs levels in the morning were measured spectrophotometrically using the glomerular filtration rate method and the fbs ( mg / l ) level was calculated from the spectrophotometrically obtained fbs levels .\n",
            "a total of 58 patients ( 61% ) completed the study . the mean age of patients was 54.0516.08 years ( range : 24 to 79 years ) with a mean fasting plasma glucose level of 7.817.24 g / dl ( range : 0.4 to 10.8 g / dl ) .\n",
            "the frequency of dysphagia in all patients was 13.12% ( 59/155 ) and 13.00% ( 69/155 ) in patients with type 1 diabetes and patients without type 1 diabetes , respectively ( p = 0.0087 ) ( fig .\n",
            "there were no statistical differences between patients with type 1 diabetes and patients without type 1 diabetes in\n",
            "---------------\n",
            "ARTICLE STARTS HERE:since the second half of the twentieth century , the growing importance of diabetes as a public health problem in developing countries has been recognized as an area of intense research . in particular , diabetes has been associated with several multifactorial and nongenetic complications , particularly those that are closely linked to peripheral and central insulin resistance . in order to ameliorate the negative clinical effects of diabetes , it is important to understand the pathogenesis of peripheral and central insulin resistance , the relationship between these factors and type 2 diabetes .\n",
            "recently , the identification of linkages between vascular risk factors such as atherosclerosis and insulin resistance has added a new dimension to the understanding of the pathogenesis of diabetes and the need for appropriate therapeutic approaches for the management of this critical issue .\n",
            "in addition , as the pathophysiology of peripheral insulin resistance is largely similar to that of central insulin resistance , diabetes , and the subsequent complications , will likely be similarly treated clinically .\n",
            "the first priority is to identify the underlying pathogenic factors . in clinical studies ,\n",
            "the importance of a baseline lipid profile as a prognostic factor for type 2 diabetes has been proven .\n",
            "a high index of triglycerides has been consistently associated with diabetes , and an elevated inflammatory marker , c - reactive protein , has also been shown to be more predictive of developing diabetes than glucose or the use of a lipid - lowering medication .\n",
            "a recent study showed that high serum c - reactive protein levels in a nondiabetic population was independent of other lipid markers , including low - density lipoprotein cholesterol , high - density lipoprotein cholesterol , and triglycerides , suggesting the importance of a high lipid background to develop type 2 diabetes .\n",
            "while the role of ldl cholesterol has been well demonstrated , the role of high - density lipoprotein cholesterol ( hdl - c ) in the development of diabetes has not been established yet .\n",
            "recently , a relationship between elevated hdl - c and diabetes has been described in a large number of studies .\n",
            "a non - hdl subclass enriched in chylomicrons and a low hdl - c allele ( the chylomicron disease allele ) have been consistently associated with a high risk of developing diabetes .\n",
            "on the other hand , a chylomicron gene polymorphism has been shown to have a potential role in the development of the type 2 diabetes phenotype .\n",
            "although the role of the chylomicron disease allele is not entirely clear in the development of type 2 diabetes , the presence of the chylomicron disease allele in a large population - based study is sufficient to suggest that the association observed in this gene may be important in the development of diabetes .\n",
            "although there is a known relation between obesity and non - small cell lung cancer ( nsclc ) , the results of a large study that assessed the association between obesity and diabetes in a large number of subjects were contradictory .\n",
            "evidence has been obtained that obesity and diabetes may be correlated , but the data on this important question are not yet complete and inconsistent .\n",
            "one of the problems in the field of diabetes research is that the pathogenesis of diabetes is a difficult and complicated issue , and even the most well - known characteristics of diabetes ( such as the insulin resistance ) may not be fully understood .\n",
            "interestingly , a number of well - established chronic diseases and their hallmark complications are also associated with insulin resistance .\n",
            "for example , according to a large international study , type 2 diabetes is often associated with cardiovascular disease , hypertension , and hyperlipidaemia .\n",
            "as is evidenced by the well - established relationship between type 2 diabetes and cardiovascular disease , the development of a type 2 diabetes is intimately linked to the development of other related conditions .\n",
            "this is particularly the case in patients with type 2 diabetes who are already suffering from cardiovascular disease or renal failure .\n",
            "the second priority is to improve the understanding of the function of insulin - related signaling pathways .\n",
            "there are few studies in the literature that have assessed the role of insulin and its related signaling pathways in the pathogenesis of diabetes .\n",
            "it has been shown that insulin signaling is involved in the pathogenesis of type 2 diabetes , and several lines of evidence indicate that insulin signaling can be used as an indicator of peripheral insulin resistance .\n",
            "postprandial hyperinsulinemia in animal studies has been reportedly involved in the etiology of type 2 diabetes , as has been analyzed via in vitro genetic approaches .\n",
            "in addition , it has been demonstrated that the expression of insulin receptors , which have been shown to be upregulated in the pancreas of insulin resistance , is induced by the presence of type 2 diabetes .\n",
            "furthermore , it has been demonstrated that the presence of the insulin - signaling pathway is increased in diabetic cells and tissues , and the stimulation of insulin signaling pathways is mediated by two different types of insulin receptor proteins , the f - type and the thr - type .\n",
            "considering that the insulin signaling pathways are tightly interconnected , it is not surprising\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aFPw66H-WoY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}